{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz2j3gjYBvmq"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAshRyTWBf-Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBzMmxjnEUZJ"
   },
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8rpsTShuNaJ",
    "outputId": "e8cae389-45ce-4798-864d-fd481431d9b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h63oSRyMGHxt"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/NLP_project/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvLVg-iIGIIU",
    "outputId": "2dee269d-96bb-4bb4-cb56-9e18b3fe0ba8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Define text cleaner function\n",
    "def text_cleaner(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs with _url_\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '_url_', text)\n",
    "\n",
    "    # Replace emails with _email_\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '_email_', text)\n",
    "\n",
    "    # Remove special characters except ', \", _url_, and _email_\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s'\\\"_]\", '', text)\n",
    "\n",
    "    # Remove sequences of underscores that are not _url_ or _email_\n",
    "    def clean_underscores(match):\n",
    "        text = match.group(0)\n",
    "        if text in ['_url_', '_email_']:\n",
    "            return text\n",
    "        return text.replace('_', '')\n",
    "\n",
    "    text = re.sub(r'\\b_[a-zA-Z0-9_]*_\\b', clean_underscores, text)\n",
    "\n",
    "    # Replace sequences of whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/content/drive/MyDrive/NLP_project/phishing_data/phishing_email.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Clean the email texts\n",
    "df['text_combined'] = df['text_combined'].apply(text_cleaner)\n",
    "\n",
    "# Ensure labels are integers\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text_combined'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = EmailDataset(train_texts, train_labels, tokenizer, max_len=512)\n",
    "val_dataset = EmailDataset(val_texts, val_labels, tokenizer, max_len=512)\n",
    "data_set = EmailDataset(df['text_combined'].tolist(), df['label'].tolist(), tokenizer, max_len=512)\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model by saving it temporarily.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / (1024 ** 2)\n",
    "    print(f\"Model size: {size:.2f} MB\")\n",
    "    os.remove(\"temp.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl4Ucqwer-UX"
   },
   "source": [
    "##**Roberta For Sequence Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDOMh2TRUZOw"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711,
     "referenced_widgets": [
      "242817432ec64c65ae0092eaa57b0b28",
      "c888cefa52a44907858d7ef96e9c4046",
      "793cbea070ab4faca6e3e17eb129e5fb",
      "8ff25d74ba514708b3ee362b56ef3e2d",
      "211445502d934ce9a32d45f07443c3b4",
      "968cabbcbdff41b4bd3afd2911e7b2ee",
      "0102bfbd68bd411194eb6a9233464607",
      "ae2cb29a2b324629b6bf521b24ef11ee",
      "82d59f5b21ba46a8928c1c2ebe3a00b1",
      "196ef8bf48a04a6993efb0db48edde60",
      "05e35f412e7d4d5884265d62641e4f88",
      "68362ff0b054417c83aa7b0c31379a97",
      "1ceaf585a905411a89485a8d3a940aa6",
      "610eac5e5abc403aa57eaeca7df11e35",
      "7295d19ef7ca4384a7b7501f8d29e915",
      "59b49e23d22f4316b87c61687aea8542",
      "0babbf5feec1445a81d231151e0d5e77",
      "c295b77221bd42fcbeb4cd0d1d23b61d",
      "4ddae419281b4414be628c58c88bc444",
      "daf3e5ceb7204da08b9d751b9aeeee82",
      "36f5e12650dc414d9af98c8146f478e1",
      "a0ffda0894874241940c84cc5e6c6a8c",
      "ceafc6bdc70c48e093acb0b068c6e10a",
      "4bb4894ac49b4073b5add5cf60310ad8",
      "472db68a7cf8426597c768d83047f474",
      "b5689b9f66ef46e2bd34c92d3ebb3077",
      "684fe0013ef344f28cb39f57f5dc2e41",
      "e14968a1916242f7b7ea9f1226f9da38",
      "18199459dda04eb9b3f13dcd645f2d93",
      "98f63712498f4bf698728a899045b915",
      "a88ecf8d445449ecae942377e7e4edf2",
      "c8f1931f21c8495e86025391daeb3184",
      "ccd317d11b9041bd9fef0a5c644a2e10",
      "e387f74302364bbdbf63aae4e648ad8b",
      "5f77d7db3b5c42b49f0a30f8579ed8a1",
      "e99ef5310adb43fca90d1300149aff07",
      "edecbe4e701348e49f7c4d461554e40d",
      "f0434966f0944840bba888b24fc05024",
      "6f00b599c85a4415a0c892ba28ae3d3f",
      "73cc3ae65cf6442697ff3347428cca75",
      "ccdee859fdcf4836b4155cf959a79980",
      "76bf606aa53e43039fb6f9cf4e15018e",
      "5e489a97a30c40aa8ee8b24e3ee0f5c2",
      "7683b095694347bf83fc4d66e6b7f8b8",
      "6e0a14a8ecd84bcd9ec70e21deb9e50d",
      "ec8a66d29a9b434ba3eb197a43a6c949",
      "6d02e0d72c454d248d662c7fc9458234",
      "53fe61b3609b43daa7f9180121a350ea",
      "2644c577cc08493f938356779d39ad7a",
      "19bd2b6b3839417a8a456e68a7f474ff",
      "ad1587a6f64e4baf8ca640d529c63aa2",
      "f75e33e001f5498b8d5d553ace5cb452",
      "25f90152cb8c415797a8c85a04494566",
      "256e6aa44ba94e82be6be54df640e7e7",
      "60b93f1ad3d54b399a2b41c9d4677ff8",
      "c8870cbc209a45299e74d4028e47b7dc",
      "bab3ca3d70634690a60fd3e422cdf7b3",
      "aaa9bc10a3454789825a732a098b929e",
      "da775165bff14fdea5a023810e3dd15c",
      "3032a4be86884bb9aaebf9bf8ca64060",
      "845ff7aa240e420ba636f1e73aab0d9b",
      "4fd22bc2d2b34e38ac0d3dee905b6ffc",
      "1e33072bcbd14bc8bf44d21a67ac64c4",
      "3d1540c6e61d4bebbe6f27a3019772cd",
      "1df1efb3eb6e40c396ae8040c2e7fb68",
      "69b492f4188941828742aa6dd757aabc"
     ]
    },
    "id": "hGlgNJNfpAoN",
    "outputId": "e572bc5f-209b-47fb-e576-b0404fffa12d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242817432ec64c65ae0092eaa57b0b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68362ff0b054417c83aa7b0c31379a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceafc6bdc70c48e093acb0b068c6e10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e387f74302364bbdbf63aae4e648ad8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0a14a8ecd84bcd9ec70e21deb9e50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8870cbc209a45299e74d4028e47b7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12375' max='12375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12375/12375 1:45:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.089579</td>\n",
       "      <td>0.976724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.064548</td>\n",
       "      <td>0.986908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>0.989756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1032' max='1032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1032/1032 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.04774054139852524\n",
      "Validation Accuracy: 98.98%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/NLP_project/trained_model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/NLP_project/trained_model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/NLP_project/trained_model/vocab.json',\n",
       " '/content/drive/MyDrive/NLP_project/trained_model/merges.txt',\n",
       " '/content/drive/MyDrive/NLP_project/trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Define compute_metrics function to calculate accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split dataset with a fixed random seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text_combined'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = EmailDataset(train_texts, train_labels, tokenizer, max_len=512)\n",
    "val_dataset = EmailDataset(val_texts, val_labels, tokenizer, max_len=512)\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define training arguments with matching save and eval strategies\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,  # Increased batch size for efficiency\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,  # Reduced warmup steps\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,  # Log every 100 steps\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate only at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model only at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    fp16=True,  # Enable mixed precision training for faster GPU training\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_output = trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation Loss: {results['eval_loss']}\")\n",
    "print(f\"Validation Accuracy: {results['eval_accuracy'] * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model and tokenizer after training completes\n",
    "model.save_pretrained('/content/drive/MyDrive/NLP_project/trained_model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/NLP_project/trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cqdToPt40vp",
    "outputId": "bb545263-1ce9-4f82-f634-bbc3e866176a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.json',\n",
       " './trained_model/merges.txt',\n",
       " './trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model's weights\n",
    "model.save_pretrained('./trained_model')\n",
    "# Optionally, save the tokenizer as well\n",
    "tokenizer.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZGD75UbsGRo"
   },
   "source": [
    "## **Bert For Sequence Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "gpfUNZtcmYbB",
    "outputId": "813047af-39e1-4b1e-cc88-146aba968484"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5387' max='12375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5387/12375 19:52 < 25:47, 4.51 it/s, Epoch 1.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.043093</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12375' max='12375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12375/12375 45:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.043093</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.994181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033964</td>\n",
       "      <td>0.995575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1032' max='1032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1032/1032 02:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Loss: 0.0339641198515892\n",
      "Validation Accuracy: 99.56%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/NLP_project/trained_modelbert/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/NLP_project/trained_modelbert/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/NLP_project/trained_modelbert/vocab.txt',\n",
       " '/content/drive/MyDrive/NLP_project/trained_modelbert/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define compute_metrics function to calculate accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = EmailDataset(df['text_combined'].tolist(), df['label'].tolist(), tokenizer, max_len=512)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision training for faster GPU training\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_output = trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation Loss: {results['eval_loss']}\")\n",
    "print(f\"Validation Accuracy: {results['eval_accuracy'] * 100:.2f}%\")\n",
    "model.save_pretrained('/content/drive/MyDrive/NLP_project/trained_modelbert')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/NLP_project/trained_modelbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm5ucaDYcYqP"
   },
   "source": [
    "# Model compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "9XxpFR3AJ1KL",
    "outputId": "23c3ca9f-2ca1-4f1b-dbc8-a9e24e1d4148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum[onnxruntime]\n",
      "  Downloading optimum-1.21.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting coloredlogs (from optimum[onnxruntime])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.13.2)\n",
      "Requirement already satisfied: transformers<4.44.0,>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[onnxruntime]) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (2.3.1+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (24.1)\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (0.23.5)\n",
      "Collecting datasets (from optimum[onnxruntime])\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting onnx (from optimum[onnxruntime])\n",
      "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime])\n",
      "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting evaluate (from optimum[onnxruntime])\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]) (3.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (3.15.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->optimum[onnxruntime])\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum[onnxruntime])\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (4.66.5)\n",
      "Collecting xxhash (from datasets->optimum[onnxruntime])\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->optimum[onnxruntime])\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->optimum[onnxruntime]) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime]) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.12.2)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (24.3.25)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum[onnxruntime])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.29.0->transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[onnxruntime]) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.29.0->transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[onnxruntime]) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<4.44.0,>=4.29.0->transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[onnxruntime]) (0.19.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[onnxruntime]) (0.1.99)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum[onnxruntime])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[onnxruntime]) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->optimum[onnxruntime]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->optimum[onnxruntime]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->optimum[onnxruntime]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->optimum[onnxruntime]) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[onnxruntime]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[onnxruntime]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[onnxruntime]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[onnxruntime]) (1.16.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m527.3/527.3 kB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.2/13.2 MB\u001B[0m \u001B[31m86.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m16.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.9/15.9 MB\u001B[0m \u001B[31m104.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading optimum-1.21.4-py3-none-any.whl (421 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m421.5/421.5 kB\u001B[0m \u001B[31m32.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.9/39.9 MB\u001B[0m \u001B[31m44.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m12.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: xxhash, pyarrow, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, coloredlogs, onnxruntime, nvidia-cusolver-cu12, datasets, evaluate, optimum\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed coloredlogs-15.0.1 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 humanfriendly-10.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnx-1.16.2 onnxruntime-1.19.0 optimum-1.21.4 pyarrow-17.0.0 xxhash-3.5.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "a3e3eb68938e47719325c6b9e43250fe",
       "pip_warning": {
        "packages": [
         "pyarrow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %pip install optimum[onnxruntime]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "inference roberta"
   ],
   "metadata": {
    "id": "ji847Bl6T0vw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "OErWJwyaA-ws",
    "outputId": "38a61f15-d29c-46d3-fe03-f024cdec3858"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2174' max='2063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2063/2063 05:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.04774187132716179\n",
      "Validation Accuracy: 98.98%\n"
     ]
    }
   ],
   "source": [
    "#Roberta\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "trainer.model = model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation Loss: {results['eval_loss']}\")\n",
    "print(f\"Validation Accuracy: {results['eval_accuracy'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model by saving it temporarily.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / (1024 ** 2)\n",
    "    print(f\"Model size: {size:.2f} MB\")\n",
    "    os.remove(\"temp.pth\")\n"
   ],
   "metadata": {
    "id": "uln6vqvQUbOW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkE-dGDVYrCF"
   },
   "source": [
    "## **Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4nnwPJZ3qne"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lppqqttmw74g",
    "outputId": "3bf3cde5-2166-4c5e-b012-8201dbba6889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 498.655353\n",
      "Size (MB): 242.135253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [1:11:05<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9888\n",
      "Average Inference Time per Batch: 2.0464 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_path = '/content/drive/MyDrive/NLP_project/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "# Define the quantization configuration\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def evaluate_model(model, val_dataset, batch_size=8):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    inference_times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Move inputs to device\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = batch['labels'].to(model.device)\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Measure end time\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate time taken for inference\n",
    "            inference_times.append(end_time - start_time)\n",
    "\n",
    "            # Get the predicted classes\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            # Update total correct and total samples\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    # Calculate average inference time per batch\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "    return accuracy, avg_inference_time\n",
    "# Assuming val_dataset is already tokenized and ready\n",
    "accuracy, avg_inference_time = evaluate_model(quantized_model, val_dataset)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Average Inference Time per Batch: {avg_inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA18rG2R89yk"
   },
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "Mnt3t-Ps6-UL",
    "outputId": "137e8620-6c5d-417c-9ac7-2467b590a132"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2063' max='2063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2063/2063 05:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model - Validation Loss: 0.07801347970962524\n",
      "Pruned Model - Validation Accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Function to prune the model\n",
    "def prune_model(model, amount=0.2):\n",
    "    \"\"\"\n",
    "    Apply structured pruning to all applicable layers of the model except the final classifier.\n",
    "    :param model: The model to be pruned.\n",
    "    :param amount: The proportion of parameters to prune.\n",
    "    :return: The pruned model.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and \"classifier\" not in name:\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, 'weight')  # Apply pruning and remove reparameterization\n",
    "\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            prune.l1_unstructured(module, name='bias', amount=amount)\n",
    "            prune.remove(module, 'bias')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prune the model\n",
    "pruned_model = prune_model(model)\n",
    "pruned_model.to(device)\n",
    "\n",
    "# Update the Trainer with the pruned model (replace with your actual trainer initialization code)\n",
    "trainer.model = pruned_model\n",
    "\n",
    "# Evaluate the pruned model\n",
    "pruned_results = trainer.evaluate()\n",
    "print(f\"Pruned Model - Validation Loss: {pruned_results['eval_loss']}\")\n",
    "print(f\"Pruned Model - Validation Accuracy: {pruned_results['eval_accuracy'] * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9Lu77JE6-JO",
    "outputId": "065379d1-ef34-4a6b-f195-c7e9fa371dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero weights: 773 / 124544256 (0.00% sparsity)\n",
      "--------------------------------------------------------------------\n",
      "Number of zero weights: 24812544 / 124544256 (19.92% sparsity)\n"
     ]
    }
   ],
   "source": [
    "# Function to count zero weights in the model\n",
    "def count_zero_weights(model):\n",
    "    zero_weights = 0\n",
    "    total_weights = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            zero_weights += torch.sum(param == 0).item()\n",
    "            total_weights += param.nelement()\n",
    "    return zero_weights, total_weights\n",
    "\n",
    "# Prune the model\n",
    "model_path = '/content/drive/MyDrive/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "# Count zero weights in the pruned model\n",
    "zero_weights, total_weights = count_zero_weights(model)\n",
    "print(f\"Number of zero weights: {zero_weights} / {total_weights} ({(zero_weights / total_weights) * 100:.2f}% sparsity)\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "zero_weights, total_weights = count_zero_weights(pruned_model)\n",
    "print(f\"Number of zero weights: {zero_weights} / {total_weights} ({(zero_weights / total_weights) * 100:.2f}% sparsity)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nQEyZs67n9C",
    "outputId": "6e658f77-204b-45cb-e60f-b9cb6071fd89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prune model\n",
      "Total parameters: 124647170\n",
      "Non-zero parameters: 99814014\n",
      "Zero parameters: 24833156\n",
      "Model sparsity: 19.92%\n",
      "--------------------------------------\n",
      "Original model\n",
      "Total parameters: 124647170\n",
      "Non-zero parameters: 124646397\n",
      "Zero parameters: 773\n",
      "Model sparsity: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the total number of trainable parameters and the number of non-zero parameters in the model.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_zero_params = sum(torch.sum(p != 0).item() for p in model.parameters() if p.requires_grad)\n",
    "    zero_params = total_params - non_zero_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Non-zero parameters: {non_zero_params}\")\n",
    "    print(f\"Zero parameters: {zero_params}\")\n",
    "    print(f\"Model sparsity: {(zero_params / total_params) * 100:.2f}%\")\n",
    "\n",
    "# Prune the model\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Prune model\")\n",
    "print_model_parameters(pruned_model)\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Original model\")\n",
    "model_path = '/content/drive/MyDrive/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print_model_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "W6AQWlUd7qB6",
    "outputId": "fe51ed31-d188-49b6-c887-18a21335efe2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Get the total number of trainable parameters, non-zero parameters, and zero parameters in the model.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    :return: A tuple containing (total_params, non_zero_params, zero_params).\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_zero_params = sum(torch.sum(p != 0).item() for p in model.parameters() if p.requires_grad)\n",
    "    zero_params = total_params - non_zero_params\n",
    "    return total_params, non_zero_params, zero_params\n",
    "\n",
    "def plot_parameters_comparison(original_params, pruned_params):\n",
    "    \"\"\"\n",
    "    Plot a comparison of total, non-zero, and zero parameters between the original and pruned models.\n",
    "\n",
    "    :param original_params: A tuple containing (total_params, non_zero_params, zero_params) for the original model.\n",
    "    :param pruned_params: A tuple containing (total_params, non_zero_params, zero_params) for the pruned model.\n",
    "    \"\"\"\n",
    "    labels = ['Total Parameters', 'Non-Zero Parameters', 'Zero Parameters']\n",
    "    original_values = [original_params[0], original_params[1], original_params[2]]\n",
    "    pruned_values = [pruned_params[0], pruned_params[1], pruned_params[2]]\n",
    "\n",
    "    x = range(len(labels))\n",
    "    width = 0.35  # Width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, original_values, width, label='Original Model')\n",
    "    ax.bar([p + width for p in x], pruned_values, width, label='Pruned Model')\n",
    "\n",
    "    ax.set_ylabel('Number of Parameters')\n",
    "    ax.set_title('Comparison of Model Parameters: Original vs Pruned')\n",
    "    ax.set_xticks([p + width / 2 for p in x])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load the original model\n",
    "model_path = '/content/drive/MyDrive/trained_model'\n",
    "original_model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Get parameters for the original model\n",
    "original_params = get_model_parameters(original_model)\n",
    "\n",
    "\n",
    "# Get parameters for the pruned model\n",
    "pruned_params = get_model_parameters(pruned_model)\n",
    "\n",
    "# Plot the comparison\n",
    "plot_parameters_comparison(original_params, pruned_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhD7SQII8KTu",
    "outputId": "b21bd3e0-f93d-48c2-bc6e-356b04c83481",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model:\n",
      "Layer: roberta.embeddings.word_embeddings.weight\n",
      "  Total parameters: 38603520\n",
      "  Non-zero parameters: 30882816\n",
      "  Zero parameters: 7720704\n",
      "  Layer sparsity: 20.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.position_embeddings.weight\n",
      "  Total parameters: 394752\n",
      "  Non-zero parameters: 315802\n",
      "  Zero parameters: 78950\n",
      "  Layer sparsity: 20.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.token_type_embeddings.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 471552\n",
      "  Zero parameters: 118272\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1887744\n",
      "  Zero parameters: 471552\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 2458\n",
      "  Zero parameters: 614\n",
      "  Layer sparsity: 19.99%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 1886208\n",
      "  Zero parameters: 473088\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 614\n",
      "  Zero parameters: 154\n",
      "  Layer sparsity: 20.05%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.out_proj.weight\n",
      "  Total parameters: 1536\n",
      "  Non-zero parameters: 1536\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.out_proj.bias\n",
      "  Total parameters: 2\n",
      "  Non-zero parameters: 2\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Summary for the entire model:\n",
      "Total parameters: 124647170\n",
      "Non-zero parameters: 99814014\n",
      "Zero parameters: 24833156\n",
      "Model sparsity: 19.92%\n",
      "--------------------------------------\n",
      "Original Model:\n",
      "Layer: roberta.embeddings.word_embeddings.weight\n",
      "  Total parameters: 38603520\n",
      "  Non-zero parameters: 38603515\n",
      "  Zero parameters: 5\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.position_embeddings.weight\n",
      "  Total parameters: 394752\n",
      "  Non-zero parameters: 393984\n",
      "  Zero parameters: 768\n",
      "  Layer sparsity: 0.19%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.token_type_embeddings.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.embeddings.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.query.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.query.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.key.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.key.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.value.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.value.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.intermediate.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.intermediate.dense.bias\n",
      "  Total parameters: 3072\n",
      "  Non-zero parameters: 3072\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.dense.weight\n",
      "  Total parameters: 2359296\n",
      "  Non-zero parameters: 2359296\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.dense.weight\n",
      "  Total parameters: 589824\n",
      "  Non-zero parameters: 589824\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.dense.bias\n",
      "  Total parameters: 768\n",
      "  Non-zero parameters: 768\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.out_proj.weight\n",
      "  Total parameters: 1536\n",
      "  Non-zero parameters: 1536\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Layer: classifier.out_proj.bias\n",
      "  Total parameters: 2\n",
      "  Non-zero parameters: 2\n",
      "  Zero parameters: 0\n",
      "  Layer sparsity: 0.00%\n",
      "--------------------------------------------------\n",
      "Summary for the entire model:\n",
      "Total parameters: 124647170\n",
      "Non-zero parameters: 124646397\n",
      "Zero parameters: 773\n",
      "Model sparsity: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_model_parameters_by_layer(model):\n",
    "    \"\"\"\n",
    "    Print the total number of trainable parameters and the number of non-zero parameters in each layer of the model.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    non_zero_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            layer_total = param.numel()\n",
    "            layer_non_zero = torch.sum(param != 0).item()\n",
    "            layer_zero = layer_total - layer_non_zero\n",
    "\n",
    "            total_params += layer_total\n",
    "            non_zero_params += layer_non_zero\n",
    "            zero_params += layer_zero\n",
    "\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"  Total parameters: {layer_total}\")\n",
    "            print(f\"  Non-zero parameters: {layer_non_zero}\")\n",
    "            print(f\"  Zero parameters: {layer_zero}\")\n",
    "            print(f\"  Layer sparsity: {(layer_zero / layer_total) * 100:.2f}%\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "    print(\"Summary for the entire model:\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Non-zero parameters: {non_zero_params}\")\n",
    "    print(f\"Zero parameters: {zero_params}\")\n",
    "    print(f\"Model sparsity: {(zero_params / total_params) * 100:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assume pruned_model is the pruned model and model_path is the path to the original model\n",
    "\n",
    "print(\"Pruned Model:\")\n",
    "print_model_parameters_by_layer(pruned_model)\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "# Load the original model\n",
    "model_path = '/content/drive/MyDrive/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Original Model:\")\n",
    "print_model_parameters_by_layer(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yldH20VS8Ndd",
    "outputId": "746ef0f9-0570-41f5-807d-22bab69628dc",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Dead Neurons:\n",
      "Pruned Model Dead Neurons:\n",
      "Layer: roberta.encoder.layer.0.attention.self.query\n",
      "  Dead Neurons: [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 28, 30, 31, 33, 35, 38, 39, 42, 43, 45, 46, 47, 48, 50, 51, 53, 56, 57, 58, 59, 61, 62, 65, 67, 68, 70, 72, 73, 74, 78, 79, 80, 81, 85, 88, 89, 91, 92, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 111, 112, 113, 114, 115, 116, 118, 119, 120, 123, 124, 125, 126, 127, 396, 398, 405, 409, 414, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 669]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.key\n",
      "  Dead Neurons: [1, 6, 8, 12, 13, 15, 18, 19, 21, 25, 26, 31, 35, 37, 40, 41, 42, 43, 45, 46, 47, 48, 50, 53, 56, 57, 59, 61, 63, 64, 66, 68, 69, 70, 71, 73, 76, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 111, 119, 120, 121, 124, 125, 127, 163, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 640, 641, 647, 650, 653, 655, 660, 665, 669, 670, 671, 677, 680, 682, 688, 689, 690, 691, 701, 703]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.self.value\n",
      "  Dead Neurons: [0, 1, 7, 9, 10, 17, 18, 21, 23, 27, 29, 32, 40, 41, 44, 46, 49, 55, 58, 286, 333, 386, 394, 399, 406, 419, 442, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.attention.output.dense\n",
      "  Dead Neurons: [10, 13, 16, 19, 20, 22, 24, 31, 34, 41, 46, 47, 50, 59, 63, 69, 73, 76, 81, 84, 95, 106, 110, 113, 117, 119, 123, 124, 131, 132, 136, 140, 155, 166, 169, 171, 182, 185, 196, 202, 204, 206, 214, 215, 218, 222, 229, 232, 247, 249, 250, 251, 252, 254, 261, 262, 265, 273, 287, 291, 295, 304, 308, 326, 333, 338, 344, 345, 348, 355, 359, 364, 367, 373, 378, 382, 386, 387, 396, 398, 408, 409, 411, 414, 416, 417, 418, 434, 451, 458, 461, 469, 470, 473, 486, 492, 505, 516, 517, 520, 526, 529, 540, 548, 552, 556, 564, 569, 573, 583, 607, 608, 609, 611, 618, 623, 624, 638, 641, 643, 652, 660, 661, 663, 674, 675, 676, 677, 678, 690, 693, 694, 696, 701, 705, 707, 712, 718, 720, 722, 724, 728, 733, 734, 735, 736, 738, 740, 745, 752, 758, 759, 762, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.intermediate.dense\n",
      "  Dead Neurons: [10, 27, 30, 41, 44, 46, 70, 71, 73, 78, 95, 98, 105, 108, 122, 124, 130, 135, 136, 141, 142, 146, 149, 152, 157, 160, 163, 166, 168, 171, 174, 182, 184, 187, 195, 196, 197, 207, 210, 213, 219, 220, 222, 230, 244, 248, 252, 253, 255, 264, 271, 273, 275, 278, 285, 286, 291, 297, 307, 308, 310, 313, 317, 324, 338, 342, 343, 350, 356, 358, 362, 368, 369, 370, 376, 392, 393, 415, 417, 422, 432, 442, 454, 457, 459, 470, 471, 479, 484, 494, 501, 503, 510, 511, 516, 519, 522, 528, 529, 534, 535, 545, 549, 556, 557, 562, 563, 565, 572, 578, 599, 602, 611, 615, 621, 627, 630, 631, 639, 643, 650, 662, 670, 671, 675, 681, 688, 691, 692, 694, 698, 699, 722, 736, 745, 748, 755, 757, 759, 763, 774, 779, 787, 793, 797, 809, 814, 816, 819, 820, 825, 829, 835, 838, 839, 840, 843, 845, 850, 854, 857, 858, 861, 864, 868, 885, 886, 887, 895, 900, 902, 904, 906, 911, 922, 923, 925, 930, 934, 940, 948, 949, 952, 956, 963, 967, 971, 974, 980, 984, 985, 988, 990, 992, 1003, 1008, 1010, 1012, 1020, 1024, 1026, 1028, 1030, 1034, 1040, 1045, 1055, 1059, 1063, 1070, 1072, 1082, 1088, 1093, 1097, 1099, 1109, 1112, 1122, 1126, 1131, 1141, 1157, 1158, 1162, 1167, 1168, 1172, 1199, 1203, 1204, 1207, 1216, 1219, 1221, 1230, 1234, 1239, 1240, 1248, 1252, 1253, 1258, 1275, 1287, 1292, 1296, 1298, 1299, 1305, 1316, 1320, 1324, 1330, 1336, 1356, 1362, 1363, 1369, 1370, 1374, 1376, 1379, 1380, 1388, 1389, 1391, 1393, 1395, 1407, 1417, 1419, 1428, 1433, 1443, 1447, 1458, 1460, 1462, 1464, 1466, 1467, 1470, 1474, 1479, 1480, 1483, 1485, 1496, 1500, 1517, 1520, 1525, 1535, 1536, 1539, 1541, 1542, 1544, 1545, 1547, 1560, 1563, 1567, 1570, 1572, 1573, 1580, 1587, 1591, 1599, 1606, 1609, 1612, 1617, 1621, 1624, 1633, 1637, 1638, 1641, 1642, 1646, 1651, 1654, 1663, 1666, 1670, 1674, 1675, 1676, 1681, 1684, 1703, 1706, 1707, 1710, 1715, 1726, 1730, 1731, 1736, 1744, 1745, 1750, 1752, 1763, 1764, 1769, 1771, 1776, 1778, 1787, 1789, 1802, 1808, 1815, 1824, 1829, 1834, 1838, 1840, 1844, 1847, 1855, 1861, 1882, 1883, 1885, 1899, 1906, 1911, 1914, 1915, 1916, 1920, 1921, 1924, 1931, 1933, 1940, 1944, 1948, 1953, 1955, 1956, 1958, 1961, 1964, 1967, 1974, 1980, 1987, 1999, 2001, 2005, 2016, 2027, 2047, 2048, 2053, 2055, 2057, 2063, 2065, 2067, 2069, 2070, 2076, 2080, 2082, 2084, 2087, 2091, 2096, 2102, 2110, 2111, 2115, 2118, 2122, 2132, 2143, 2161, 2163, 2165, 2174, 2178, 2180, 2185, 2186, 2187, 2198, 2204, 2208, 2217, 2221, 2225, 2226, 2227, 2241, 2246, 2248, 2249, 2252, 2257, 2260, 2266, 2267, 2279, 2281, 2282, 2284, 2285, 2296, 2305, 2313, 2318, 2323, 2324, 2326, 2333, 2337, 2338, 2343, 2350, 2355, 2356, 2357, 2358, 2361, 2362, 2365, 2366, 2372, 2374, 2379, 2383, 2385, 2388, 2393, 2396, 2399, 2405, 2406, 2407, 2410, 2414, 2426, 2431, 2433, 2434, 2444, 2447, 2449, 2450, 2455, 2457, 2460, 2463, 2464, 2465, 2469, 2473, 2475, 2477, 2492, 2515, 2522, 2525, 2526, 2548, 2554, 2560, 2576, 2583, 2597, 2598, 2599, 2600, 2607, 2609, 2613, 2615, 2616, 2621, 2628, 2634, 2646, 2649, 2657, 2659, 2660, 2662, 2672, 2688, 2694, 2695, 2698, 2700, 2701, 2704, 2707, 2718, 2722, 2737, 2738, 2745, 2749, 2775, 2777, 2783, 2785, 2791, 2797, 2801, 2813, 2814, 2815, 2824, 2826, 2828, 2831, 2833, 2834, 2839, 2840, 2850, 2864, 2866, 2882, 2889, 2890, 2899, 2903, 2909, 2910, 2914, 2920, 2921, 2925, 2929, 2931, 2935, 2938, 2940, 2942, 2948, 2951, 2952, 2959, 2960, 2970, 2972, 2975, 2987, 2988, 2989, 2990, 2991, 3004, 3007, 3008, 3014, 3024, 3029, 3033, 3037, 3042, 3046, 3052, 3057, 3064, 3069]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.0.output.dense\n",
      "  Dead Neurons: [13, 14, 15, 22, 24, 29, 34, 38, 41, 46, 47, 60, 61, 63, 69, 73, 75, 77, 79, 81, 82, 84, 85, 87, 93, 105, 106, 110, 119, 122, 123, 124, 131, 141, 144, 148, 155, 156, 157, 158, 161, 166, 169, 171, 180, 185, 193, 195, 202, 204, 206, 211, 214, 215, 225, 229, 231, 232, 240, 247, 254, 261, 265, 278, 287, 300, 302, 327, 330, 338, 340, 348, 355, 359, 367, 373, 377, 378, 390, 397, 398, 403, 405, 406, 408, 409, 411, 414, 416, 417, 428, 451, 468, 486, 491, 492, 496, 497, 503, 518, 519, 529, 538, 540, 550, 551, 561, 569, 573, 577, 579, 583, 586, 604, 606, 609, 611, 619, 623, 624, 634, 641, 647, 652, 656, 660, 661, 662, 664, 676, 677, 678, 689, 694, 707, 718, 720, 724, 728, 731, 733, 735, 740, 742, 745, 748, 750, 751, 752, 754, 756, 759, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.query\n",
      "  Dead Neurons: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 66, 71, 73, 74, 75, 76, 86, 91, 95, 97, 102, 103, 104, 107, 113, 119, 121, 208, 224, 311, 348, 450, 457, 462, 466, 468, 472, 475, 480, 483, 487, 488, 496, 497, 500, 501, 504, 508, 640, 641, 642, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 660, 661, 662, 663, 664, 666, 667, 668, 670, 671, 672, 673, 675, 676, 677, 678, 679, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 703, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.key\n",
      "  Dead Neurons: [1, 2, 3, 9, 11, 12, 14, 18, 20, 22, 25, 27, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 61, 86, 89, 97, 127, 322, 329, 338, 339, 342, 345, 347, 348, 352, 364, 366, 368, 373, 379, 450, 451, 452, 453, 454, 455, 457, 459, 460, 463, 465, 466, 467, 468, 469, 471, 472, 473, 474, 477, 478, 479, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 495, 496, 497, 499, 500, 501, 502, 504, 505, 508, 509, 511, 587, 643, 644, 648, 649, 650, 651, 652, 653, 654, 656, 657, 658, 659, 660, 661, 664, 666, 667, 668, 669, 670, 672, 673, 674, 675, 676, 677, 678, 680, 681, 682, 683, 684, 685, 687, 688, 690, 691, 692, 693, 695, 696, 698, 700, 701, 703, 718, 728, 747, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.self.value\n",
      "  Dead Neurons: [1, 2, 5, 7, 8, 11, 12, 18, 21, 22, 23, 24, 25, 26, 27, 30, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 47, 48, 51, 52, 53, 57, 58, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 423, 537, 559, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 593, 594, 595, 596, 597, 598, 599, 601, 603, 604, 605, 606, 608, 609, 610, 611, 612, 613, 614, 616, 617, 621, 622, 623, 624, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 15, 16, 17, 19, 24, 29, 34, 38, 41, 46, 47, 50, 55, 57, 61, 63, 67, 75, 82, 87, 93, 95, 100, 105, 106, 119, 122, 123, 124, 125, 131, 133, 136, 144, 157, 159, 161, 169, 176, 180, 185, 187, 193, 195, 202, 206, 211, 213, 214, 218, 224, 225, 231, 232, 235, 240, 245, 247, 249, 251, 254, 265, 278, 282, 287, 300, 302, 306, 324, 327, 330, 333, 340, 348, 363, 367, 373, 378, 387, 397, 405, 408, 409, 411, 416, 417, 428, 432, 446, 451, 452, 468, 491, 492, 498, 503, 516, 518, 519, 526, 529, 538, 540, 546, 550, 561, 564, 570, 573, 574, 577, 578, 583, 586, 606, 609, 611, 623, 624, 641, 647, 652, 653, 655, 656, 664, 672, 676, 678, 683, 689, 694, 699, 707, 716, 718, 720, 728, 733, 735, 741, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.intermediate.dense\n",
      "  Dead Neurons: [0, 2, 12, 24, 29, 32, 35, 44, 45, 47, 48, 49, 51, 58, 68, 69, 71, 78, 80, 85, 88, 89, 97, 100, 101, 103, 126, 128, 129, 130, 132, 137, 138, 146, 147, 150, 153, 165, 166, 171, 179, 188, 190, 191, 194, 200, 202, 209, 219, 227, 234, 235, 252, 253, 259, 262, 269, 288, 289, 300, 302, 307, 310, 319, 322, 324, 325, 327, 329, 332, 333, 339, 340, 341, 348, 350, 358, 359, 366, 369, 378, 385, 386, 388, 390, 391, 392, 393, 397, 398, 406, 409, 411, 416, 417, 420, 430, 431, 435, 450, 457, 459, 472, 474, 476, 479, 481, 483, 484, 485, 486, 498, 502, 503, 505, 510, 511, 515, 518, 521, 523, 525, 527, 531, 542, 543, 547, 550, 563, 572, 574, 581, 594, 601, 605, 606, 611, 627, 629, 630, 631, 634, 635, 639, 643, 644, 645, 652, 654, 676, 682, 684, 691, 695, 697, 698, 701, 703, 705, 724, 725, 728, 736, 737, 742, 747, 753, 757, 761, 766, 772, 778, 784, 786, 795, 797, 799, 807, 815, 816, 818, 822, 826, 834, 840, 851, 852, 868, 871, 873, 879, 881, 888, 891, 897, 902, 904, 920, 923, 929, 935, 938, 942, 943, 964, 965, 966, 970, 971, 978, 980, 989, 992, 1000, 1012, 1022, 1025, 1026, 1027, 1028, 1029, 1031, 1033, 1049, 1055, 1065, 1068, 1070, 1087, 1094, 1102, 1103, 1116, 1119, 1120, 1123, 1132, 1133, 1134, 1142, 1147, 1154, 1156, 1164, 1165, 1166, 1168, 1180, 1181, 1187, 1188, 1209, 1226, 1228, 1231, 1233, 1234, 1238, 1239, 1249, 1252, 1253, 1255, 1258, 1260, 1262, 1268, 1276, 1278, 1285, 1287, 1288, 1297, 1298, 1303, 1309, 1310, 1317, 1323, 1339, 1341, 1343, 1345, 1347, 1348, 1353, 1354, 1364, 1374, 1375, 1382, 1384, 1387, 1396, 1401, 1408, 1412, 1413, 1414, 1417, 1429, 1432, 1439, 1440, 1442, 1443, 1445, 1453, 1454, 1456, 1457, 1459, 1464, 1466, 1467, 1468, 1470, 1487, 1488, 1491, 1505, 1507, 1513, 1518, 1519, 1523, 1524, 1545, 1546, 1547, 1557, 1560, 1568, 1585, 1591, 1593, 1594, 1601, 1604, 1611, 1612, 1614, 1619, 1629, 1633, 1634, 1635, 1636, 1643, 1658, 1665, 1668, 1675, 1690, 1693, 1697, 1700, 1712, 1717, 1722, 1725, 1738, 1740, 1750, 1752, 1753, 1757, 1779, 1781, 1787, 1797, 1804, 1807, 1811, 1812, 1814, 1815, 1816, 1820, 1836, 1838, 1844, 1863, 1866, 1870, 1875, 1881, 1891, 1892, 1900, 1901, 1902, 1908, 1914, 1917, 1922, 1939, 1942, 1946, 1947, 1950, 1955, 1958, 1962, 1965, 1966, 1969, 1971, 1987, 1990, 1992, 1995, 1996, 2010, 2013, 2014, 2016, 2019, 2023, 2024, 2033, 2043, 2045, 2048, 2049, 2059, 2070, 2075, 2079, 2080, 2098, 2109, 2118, 2119, 2126, 2127, 2128, 2136, 2139, 2141, 2142, 2143, 2148, 2155, 2171, 2180, 2181, 2182, 2186, 2191, 2199, 2202, 2203, 2211, 2217, 2224, 2238, 2242, 2246, 2248, 2257, 2260, 2261, 2262, 2269, 2275, 2276, 2291, 2296, 2299, 2306, 2310, 2316, 2318, 2319, 2321, 2322, 2324, 2326, 2329, 2335, 2336, 2338, 2339, 2341, 2345, 2346, 2347, 2356, 2357, 2359, 2378, 2380, 2391, 2393, 2398, 2402, 2404, 2420, 2421, 2425, 2428, 2430, 2451, 2463, 2464, 2467, 2473, 2475, 2488, 2499, 2508, 2509, 2511, 2512, 2525, 2532, 2533, 2544, 2554, 2559, 2580, 2587, 2595, 2597, 2603, 2606, 2608, 2609, 2616, 2619, 2623, 2625, 2630, 2636, 2646, 2649, 2652, 2668, 2670, 2678, 2679, 2683, 2687, 2696, 2700, 2705, 2712, 2713, 2718, 2720, 2727, 2735, 2739, 2745, 2747, 2755, 2759, 2766, 2778, 2779, 2782, 2785, 2790, 2792, 2795, 2798, 2800, 2803, 2818, 2819, 2828, 2829, 2839, 2852, 2858, 2862, 2865, 2867, 2877, 2878, 2885, 2886, 2888, 2889, 2892, 2895, 2905, 2916, 2917, 2919, 2920, 2921, 2924, 2937, 2949, 2950, 2955, 2956, 2973, 2995, 2997, 3007, 3010, 3014, 3019, 3028, 3043, 3045, 3057, 3062, 3063, 3064, 3065]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.1.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 17, 19, 24, 29, 38, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 82, 85, 87, 93, 95, 100, 105, 106, 110, 119, 122, 123, 124, 125, 131, 133, 144, 148, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 204, 206, 211, 214, 215, 217, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 265, 278, 287, 302, 306, 324, 327, 330, 333, 340, 359, 361, 366, 373, 376, 378, 387, 397, 405, 408, 409, 411, 416, 417, 428, 432, 452, 453, 468, 491, 492, 496, 497, 498, 518, 519, 529, 538, 540, 546, 551, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 683, 689, 718, 720, 728, 731, 733, 735, 742, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.query\n",
      "  Dead Neurons: [3, 6, 18, 27, 37, 54, 60, 70, 98, 192, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 250, 251, 252, 253, 254, 255, 320, 324, 326, 332, 335, 336, 340, 341, 343, 344, 345, 347, 348, 351, 355, 356, 358, 359, 360, 361, 362, 363, 365, 366, 368, 369, 370, 371, 373, 377, 382, 448, 449, 450, 452, 453, 454, 456, 457, 458, 459, 460, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482, 483, 484, 485, 486, 487, 488, 489, 490, 493, 494, 495, 496, 497, 498, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 577, 601, 654, 678, 690, 700]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.key\n",
      "  Dead Neurons: [29, 31, 193, 196, 197, 198, 200, 201, 202, 203, 204, 205, 208, 209, 211, 212, 213, 214, 216, 217, 218, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 250, 251, 252, 253, 254, 255, 304, 320, 322, 323, 327, 329, 331, 333, 336, 338, 341, 343, 345, 348, 349, 350, 355, 356, 358, 359, 360, 361, 362, 363, 365, 366, 367, 370, 371, 374, 376, 379, 382, 383, 443, 449, 451, 452, 454, 455, 456, 457, 458, 459, 460, 462, 464, 465, 467, 468, 471, 472, 473, 474, 475, 476, 479, 482, 483, 485, 486, 487, 488, 490, 491, 493, 494, 498, 499, 500, 501, 502, 503, 504, 505, 507, 509, 510, 559, 641, 644, 645, 646, 654, 655, 659, 661, 663, 665, 666, 667, 668, 677, 678, 679, 681, 685, 686, 690, 692, 699, 700, 710]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.self.value\n",
      "  Dead Neurons: [65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 101, 102, 104, 105, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 411, 413, 576, 577, 579, 581, 584, 585, 586, 588, 589, 590, 592, 593, 594, 596, 599, 600, 602, 603, 604, 605, 607, 609, 614, 616, 618, 619, 620, 621, 625, 626, 628, 629, 630, 631, 634, 636, 639]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 17, 19, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 75, 82, 85, 87, 90, 93, 95, 99, 100, 105, 106, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 206, 211, 213, 214, 215, 218, 224, 225, 229, 231, 232, 235, 240, 245, 247, 251, 254, 265, 278, 282, 287, 300, 302, 306, 324, 326, 327, 330, 333, 340, 342, 359, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 411, 416, 417, 428, 432, 452, 453, 468, 491, 492, 498, 518, 519, 526, 529, 538, 540, 546, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 683, 689, 694, 699, 716, 720, 728, 731, 733, 735, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.intermediate.dense\n",
      "  Dead Neurons: [4, 11, 12, 13, 16, 18, 25, 31, 36, 41, 43, 54, 56, 59, 61, 64, 76, 79, 85, 86, 95, 98, 101, 106, 108, 115, 116, 120, 122, 123, 125, 141, 150, 151, 157, 167, 173, 182, 185, 189, 202, 205, 210, 222, 224, 227, 230, 231, 235, 236, 237, 240, 243, 249, 256, 259, 261, 266, 268, 271, 280, 305, 318, 320, 332, 333, 334, 336, 339, 343, 349, 365, 368, 369, 375, 378, 383, 387, 402, 412, 415, 416, 418, 422, 423, 428, 440, 442, 447, 450, 452, 454, 456, 463, 477, 478, 484, 487, 491, 494, 499, 504, 507, 511, 513, 516, 518, 523, 525, 527, 528, 529, 531, 532, 534, 537, 552, 554, 558, 571, 580, 582, 584, 594, 599, 606, 607, 610, 614, 616, 621, 623, 624, 638, 640, 645, 650, 651, 656, 659, 664, 674, 676, 680, 691, 693, 694, 697, 701, 702, 710, 711, 718, 719, 722, 723, 730, 731, 734, 735, 743, 744, 745, 752, 758, 769, 770, 773, 774, 782, 786, 788, 789, 793, 799, 805, 808, 813, 815, 827, 838, 839, 844, 852, 858, 859, 861, 868, 871, 875, 879, 888, 895, 897, 904, 905, 906, 911, 920, 922, 924, 944, 950, 959, 961, 963, 969, 986, 992, 994, 996, 1004, 1005, 1008, 1013, 1014, 1017, 1018, 1020, 1035, 1036, 1042, 1048, 1049, 1050, 1057, 1059, 1063, 1067, 1069, 1073, 1076, 1085, 1086, 1090, 1091, 1092, 1094, 1104, 1109, 1129, 1134, 1137, 1150, 1153, 1166, 1169, 1171, 1175, 1177, 1187, 1195, 1197, 1207, 1216, 1225, 1226, 1235, 1237, 1238, 1252, 1253, 1254, 1261, 1267, 1272, 1286, 1296, 1298, 1300, 1308, 1313, 1317, 1325, 1326, 1330, 1332, 1333, 1334, 1336, 1340, 1341, 1342, 1357, 1358, 1360, 1361, 1365, 1369, 1379, 1380, 1384, 1394, 1401, 1406, 1408, 1415, 1418, 1419, 1425, 1426, 1428, 1429, 1431, 1438, 1442, 1460, 1476, 1483, 1490, 1501, 1503, 1504, 1505, 1513, 1531, 1539, 1548, 1557, 1563, 1579, 1589, 1591, 1597, 1598, 1601, 1611, 1615, 1617, 1619, 1620, 1629, 1634, 1636, 1646, 1653, 1655, 1659, 1667, 1670, 1673, 1683, 1685, 1691, 1694, 1697, 1717, 1719, 1720, 1722, 1725, 1726, 1727, 1729, 1730, 1735, 1737, 1739, 1740, 1748, 1749, 1752, 1754, 1761, 1772, 1781, 1783, 1788, 1794, 1799, 1801, 1803, 1804, 1805, 1816, 1835, 1836, 1838, 1843, 1847, 1855, 1858, 1861, 1864, 1870, 1874, 1883, 1885, 1886, 1889, 1891, 1898, 1899, 1900, 1904, 1906, 1908, 1914, 1917, 1918, 1921, 1923, 1928, 1929, 1945, 1956, 1966, 1968, 1969, 1983, 1984, 1987, 1992, 1995, 1996, 2003, 2004, 2007, 2008, 2011, 2012, 2016, 2019, 2021, 2030, 2033, 2036, 2042, 2046, 2048, 2055, 2060, 2067, 2068, 2071, 2073, 2075, 2079, 2082, 2084, 2090, 2091, 2093, 2096, 2102, 2105, 2106, 2115, 2116, 2121, 2122, 2131, 2135, 2138, 2160, 2170, 2178, 2189, 2197, 2198, 2204, 2209, 2247, 2251, 2252, 2265, 2270, 2280, 2282, 2283, 2290, 2292, 2295, 2296, 2301, 2307, 2309, 2312, 2319, 2323, 2325, 2331, 2335, 2342, 2343, 2360, 2362, 2363, 2372, 2373, 2409, 2410, 2414, 2416, 2425, 2427, 2431, 2436, 2438, 2440, 2443, 2444, 2450, 2455, 2456, 2457, 2459, 2467, 2471, 2478, 2479, 2486, 2492, 2493, 2494, 2499, 2501, 2515, 2537, 2542, 2544, 2547, 2548, 2560, 2566, 2574, 2580, 2585, 2594, 2605, 2620, 2648, 2654, 2663, 2665, 2666, 2675, 2676, 2677, 2678, 2693, 2695, 2696, 2701, 2705, 2709, 2713, 2718, 2720, 2731, 2737, 2742, 2743, 2745, 2752, 2761, 2763, 2766, 2772, 2779, 2782, 2785, 2788, 2794, 2798, 2819, 2820, 2826, 2828, 2829, 2833, 2838, 2840, 2843, 2851, 2856, 2859, 2860, 2866, 2873, 2875, 2881, 2887, 2889, 2890, 2895, 2901, 2912, 2917, 2927, 2944, 2956, 2962, 2966, 2967, 2986, 2990, 2992, 2995, 2997, 3007, 3008, 3014, 3017, 3022, 3029, 3031, 3046, 3049, 3063, 3064, 3067, 3068, 3069]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.2.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 17, 19, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 77, 82, 85, 87, 93, 95, 97, 100, 105, 106, 110, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 185, 187, 193, 202, 206, 211, 214, 215, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 300, 302, 306, 324, 326, 327, 330, 333, 340, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 453, 468, 491, 492, 496, 498, 518, 519, 529, 538, 540, 546, 550, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 693, 718, 720, 728, 731, 733, 735, 742, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.query\n",
      "  Dead Neurons: [28, 69, 88, 92, 121, 387, 388, 395, 398, 407, 409, 410, 415, 427, 430, 435, 448, 449, 451, 452, 453, 454, 455, 456, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 576, 578, 579, 580, 581, 582, 585, 586, 588, 589, 590, 591, 592, 593, 594, 595, 596, 598, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 628, 629, 630, 631, 632, 634, 635, 636, 637, 638, 639, 641, 646, 650, 652, 653, 656, 662, 663, 665, 666, 667, 669, 670, 673, 676, 677, 679, 686, 688, 697, 698, 699, 700, 701, 702]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.key\n",
      "  Dead Neurons: [28, 35, 59, 106, 129, 149, 197, 200, 207, 220, 221, 231, 235, 242, 251, 252, 254, 339, 441, 449, 450, 451, 452, 453, 454, 456, 457, 458, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 480, 482, 484, 486, 488, 489, 490, 491, 492, 493, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 507, 508, 509, 510, 573, 576, 577, 578, 579, 580, 582, 584, 585, 586, 587, 588, 589, 591, 592, 593, 594, 595, 596, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 616, 619, 622, 623, 624, 625, 626, 627, 629, 630, 631, 632, 633, 637, 638, 639, 640, 641, 644, 646, 648, 650, 651, 652, 656, 662, 663, 664, 667, 670, 674, 676, 677, 678, 679, 681, 686, 688, 689, 690, 691, 692, 697, 698, 699, 700, 702, 703, 731]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.self.value\n",
      "  Dead Neurons: [55, 248, 257, 258, 259, 260, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 314, 315, 317, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 348, 351, 352, 353, 355, 356, 359, 360, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 377, 379, 380, 381, 382, 383, 384, 385, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 400, 401, 402, 403, 404, 406, 409, 410, 415, 416, 417, 418, 420, 421, 422, 423, 424, 426, 427, 429, 431, 432, 433, 434, 436, 437, 441, 442, 443, 445, 446, 574, 575, 579, 584, 594, 626, 697]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 17, 19, 29, 38, 40, 41, 46, 47, 55, 60, 61, 63, 67, 73, 75, 82, 85, 87, 90, 93, 95, 97, 100, 105, 106, 110, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 206, 211, 214, 215, 218, 224, 225, 229, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 300, 302, 306, 324, 326, 327, 330, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 468, 491, 492, 498, 518, 519, 526, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 694, 716, 718, 720, 728, 731, 733, 735, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.intermediate.dense\n",
      "  Dead Neurons: [6, 8, 12, 15, 16, 18, 23, 25, 30, 36, 49, 56, 64, 70, 73, 75, 81, 82, 85, 87, 88, 98, 115, 119, 120, 121, 137, 139, 144, 145, 146, 151, 158, 160, 162, 163, 166, 170, 175, 179, 180, 182, 196, 198, 199, 201, 218, 226, 228, 232, 233, 234, 235, 243, 244, 248, 260, 262, 273, 274, 277, 281, 284, 291, 294, 302, 304, 310, 311, 316, 319, 320, 327, 329, 330, 339, 343, 349, 353, 355, 372, 374, 381, 390, 393, 396, 397, 401, 404, 412, 415, 416, 421, 422, 423, 426, 438, 439, 441, 450, 452, 456, 462, 463, 464, 467, 469, 470, 472, 475, 477, 492, 495, 518, 519, 525, 534, 537, 545, 548, 551, 553, 554, 570, 592, 609, 611, 614, 617, 624, 630, 631, 642, 656, 667, 676, 679, 681, 684, 689, 691, 693, 694, 695, 705, 710, 716, 718, 726, 728, 731, 734, 739, 745, 748, 753, 766, 768, 772, 781, 783, 785, 787, 793, 801, 811, 820, 826, 833, 835, 842, 851, 860, 863, 868, 870, 884, 887, 902, 914, 916, 920, 923, 932, 937, 938, 943, 949, 952, 954, 962, 965, 975, 988, 992, 998, 999, 1023, 1028, 1031, 1032, 1040, 1043, 1046, 1054, 1055, 1057, 1063, 1074, 1076, 1078, 1079, 1084, 1089, 1090, 1093, 1096, 1098, 1099, 1105, 1106, 1109, 1114, 1116, 1118, 1126, 1131, 1132, 1141, 1145, 1146, 1147, 1163, 1165, 1176, 1178, 1180, 1195, 1197, 1199, 1201, 1203, 1210, 1220, 1228, 1230, 1231, 1232, 1234, 1238, 1240, 1241, 1242, 1246, 1247, 1251, 1259, 1271, 1274, 1275, 1288, 1291, 1292, 1293, 1296, 1297, 1298, 1299, 1300, 1312, 1317, 1321, 1322, 1323, 1331, 1336, 1344, 1351, 1361, 1363, 1369, 1385, 1388, 1399, 1404, 1419, 1431, 1435, 1448, 1456, 1467, 1471, 1473, 1476, 1477, 1484, 1486, 1507, 1513, 1515, 1518, 1527, 1530, 1535, 1547, 1549, 1556, 1561, 1562, 1570, 1574, 1582, 1590, 1596, 1601, 1607, 1609, 1610, 1617, 1618, 1619, 1620, 1622, 1644, 1645, 1649, 1652, 1653, 1671, 1672, 1677, 1678, 1679, 1683, 1691, 1692, 1695, 1706, 1708, 1714, 1725, 1726, 1728, 1731, 1737, 1743, 1745, 1757, 1759, 1767, 1769, 1770, 1771, 1780, 1781, 1784, 1788, 1795, 1797, 1801, 1803, 1804, 1818, 1825, 1827, 1830, 1845, 1848, 1850, 1858, 1861, 1865, 1871, 1874, 1878, 1879, 1889, 1894, 1895, 1897, 1903, 1911, 1913, 1915, 1927, 1929, 1936, 1937, 1940, 1943, 1951, 1965, 1968, 1969, 1974, 1980, 1998, 2004, 2008, 2009, 2013, 2020, 2022, 2023, 2043, 2057, 2061, 2070, 2080, 2081, 2087, 2088, 2100, 2117, 2123, 2124, 2127, 2135, 2142, 2145, 2148, 2154, 2161, 2163, 2167, 2168, 2169, 2172, 2173, 2174, 2178, 2180, 2183, 2187, 2194, 2202, 2204, 2213, 2236, 2238, 2239, 2245, 2246, 2249, 2250, 2251, 2254, 2255, 2259, 2261, 2267, 2268, 2270, 2271, 2272, 2276, 2277, 2280, 2282, 2284, 2285, 2288, 2290, 2291, 2295, 2300, 2305, 2306, 2309, 2314, 2315, 2326, 2327, 2329, 2333, 2338, 2339, 2343, 2344, 2349, 2353, 2364, 2381, 2388, 2391, 2420, 2423, 2435, 2441, 2445, 2447, 2450, 2452, 2457, 2472, 2475, 2477, 2481, 2482, 2487, 2506, 2507, 2510, 2513, 2514, 2517, 2519, 2521, 2524, 2529, 2530, 2541, 2542, 2551, 2553, 2554, 2564, 2567, 2574, 2581, 2584, 2586, 2589, 2594, 2596, 2597, 2602, 2613, 2614, 2634, 2635, 2644, 2645, 2649, 2651, 2652, 2654, 2660, 2666, 2673, 2676, 2680, 2681, 2693, 2702, 2703, 2704, 2707, 2721, 2731, 2732, 2736, 2751, 2757, 2758, 2760, 2768, 2773, 2792, 2800, 2806, 2808, 2823, 2824, 2825, 2826, 2828, 2845, 2852, 2853, 2855, 2862, 2868, 2873, 2882, 2883, 2888, 2889, 2900, 2904, 2905, 2907, 2912, 2923, 2924, 2926, 2928, 2940, 2942, 2945, 2952, 2956, 2959, 2980, 2983, 2984, 2987, 2993, 2994, 3001, 3003, 3005, 3014, 3023, 3027, 3031, 3040, 3042, 3051, 3052, 3055, 3060, 3061, 3063]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.3.output.dense\n",
      "  Dead Neurons: [3, 4, 10, 11, 13, 14, 15, 16, 17, 19, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 82, 85, 87, 90, 93, 95, 97, 100, 105, 110, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 180, 185, 187, 193, 198, 202, 204, 206, 211, 214, 215, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 330, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 453, 468, 491, 492, 498, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 718, 720, 728, 731, 733, 735, 742, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.query\n",
      "  Dead Neurons: [6, 7, 8, 9, 10, 11, 14, 15, 21, 26, 31, 38, 40, 41, 44, 45, 46, 49, 50, 51, 52, 54, 58, 61, 62, 63, 93, 129, 130, 131, 132, 134, 135, 136, 137, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 154, 157, 158, 159, 160, 162, 163, 164, 165, 167, 169, 172, 173, 175, 177, 181, 182, 183, 184, 186, 187, 188, 193, 196, 206, 207, 208, 212, 213, 214, 222, 226, 227, 230, 235, 238, 243, 246, 247, 248, 251, 255, 266, 267, 273, 287, 301, 309, 319, 460, 480, 491, 501, 642, 643, 644, 645, 646, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 677, 678, 679, 680, 681, 683, 684, 685, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.key\n",
      "  Dead Neurons: [6, 7, 8, 9, 10, 11, 13, 14, 15, 21, 22, 23, 24, 26, 30, 31, 32, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 57, 60, 61, 62, 63, 73, 106, 120, 131, 132, 135, 137, 140, 141, 142, 143, 146, 147, 148, 150, 152, 155, 157, 162, 164, 165, 167, 173, 176, 177, 178, 184, 186, 188, 193, 197, 198, 199, 201, 222, 223, 225, 226, 227, 228, 230, 239, 240, 248, 250, 253, 255, 258, 264, 294, 304, 479, 484, 487, 491, 495, 640, 641, 642, 643, 644, 645, 646, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 703]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.self.value\n",
      "  Dead Neurons: [131, 134, 138, 142, 143, 144, 151, 153, 158, 166, 168, 171, 175, 181, 182, 187, 191, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 420, 421, 423, 424, 425, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 513, 515, 517, 522, 525, 528, 533, 539, 540, 543, 547, 548, 549, 550, 551, 554, 555, 556, 557, 561, 562, 575, 598, 600, 629, 704, 706, 708, 709, 710, 711, 712, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 743, 744, 745, 746, 748, 749, 750, 751, 752, 754, 755, 756, 757, 758, 759, 761, 762, 763, 764, 765, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 19, 24, 29, 34, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 75, 82, 85, 87, 90, 93, 95, 97, 100, 105, 110, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 204, 206, 211, 214, 215, 218, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 453, 468, 491, 492, 497, 498, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 699, 716, 718, 720, 728, 731, 733, 735, 741, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.intermediate.dense\n",
      "  Dead Neurons: [7, 10, 21, 29, 31, 39, 43, 45, 51, 53, 55, 64, 81, 84, 85, 87, 98, 99, 104, 105, 106, 107, 116, 117, 123, 134, 136, 146, 148, 154, 160, 161, 168, 178, 185, 186, 197, 201, 202, 211, 214, 225, 232, 233, 238, 240, 250, 256, 261, 266, 276, 277, 283, 291, 294, 305, 311, 317, 318, 321, 322, 326, 333, 344, 346, 348, 351, 356, 371, 372, 379, 380, 381, 384, 389, 400, 406, 410, 411, 423, 437, 447, 465, 476, 479, 483, 493, 496, 502, 504, 513, 521, 532, 540, 541, 553, 561, 564, 565, 569, 570, 571, 579, 582, 588, 591, 596, 597, 608, 615, 620, 624, 627, 629, 631, 646, 649, 650, 653, 654, 658, 663, 679, 680, 682, 685, 697, 698, 709, 729, 730, 738, 739, 742, 745, 748, 750, 752, 754, 758, 764, 768, 770, 779, 786, 787, 789, 799, 812, 816, 824, 825, 826, 829, 835, 840, 844, 846, 848, 849, 850, 856, 858, 860, 864, 873, 874, 880, 889, 895, 910, 911, 920, 922, 928, 930, 944, 957, 958, 960, 964, 967, 968, 977, 987, 989, 990, 993, 998, 1006, 1012, 1014, 1016, 1019, 1020, 1021, 1027, 1033, 1036, 1037, 1039, 1041, 1043, 1054, 1058, 1069, 1075, 1076, 1082, 1093, 1097, 1098, 1107, 1109, 1113, 1119, 1126, 1133, 1134, 1137, 1140, 1147, 1154, 1166, 1174, 1178, 1185, 1198, 1213, 1220, 1222, 1223, 1228, 1230, 1236, 1243, 1246, 1249, 1257, 1287, 1291, 1292, 1295, 1298, 1300, 1303, 1308, 1309, 1311, 1312, 1314, 1316, 1327, 1336, 1344, 1347, 1350, 1377, 1382, 1386, 1389, 1400, 1401, 1402, 1409, 1415, 1417, 1418, 1420, 1425, 1428, 1435, 1440, 1445, 1446, 1453, 1456, 1457, 1462, 1470, 1473, 1474, 1481, 1482, 1506, 1512, 1519, 1523, 1525, 1527, 1531, 1537, 1540, 1553, 1571, 1580, 1581, 1584, 1593, 1615, 1616, 1619, 1620, 1621, 1630, 1635, 1638, 1644, 1645, 1650, 1651, 1656, 1658, 1661, 1662, 1665, 1666, 1667, 1670, 1671, 1680, 1683, 1686, 1694, 1696, 1699, 1703, 1705, 1707, 1710, 1720, 1722, 1724, 1738, 1743, 1744, 1745, 1755, 1756, 1758, 1775, 1783, 1786, 1790, 1792, 1794, 1796, 1805, 1809, 1811, 1813, 1814, 1818, 1821, 1826, 1830, 1832, 1837, 1838, 1840, 1847, 1850, 1859, 1861, 1862, 1868, 1871, 1874, 1877, 1891, 1893, 1895, 1896, 1908, 1909, 1924, 1933, 1934, 1941, 1953, 1960, 1963, 1978, 1979, 1982, 1990, 1993, 1994, 2000, 2007, 2008, 2018, 2025, 2026, 2035, 2036, 2039, 2041, 2051, 2053, 2057, 2061, 2067, 2068, 2069, 2070, 2072, 2083, 2088, 2090, 2093, 2097, 2105, 2114, 2116, 2117, 2135, 2141, 2144, 2147, 2149, 2150, 2152, 2180, 2181, 2189, 2192, 2193, 2197, 2203, 2205, 2208, 2213, 2223, 2227, 2231, 2246, 2253, 2256, 2257, 2262, 2275, 2286, 2287, 2292, 2294, 2305, 2306, 2313, 2323, 2340, 2342, 2348, 2349, 2354, 2359, 2364, 2365, 2367, 2368, 2370, 2379, 2381, 2384, 2394, 2396, 2399, 2403, 2406, 2411, 2417, 2418, 2442, 2444, 2447, 2453, 2454, 2466, 2481, 2482, 2490, 2493, 2497, 2498, 2499, 2503, 2504, 2507, 2509, 2512, 2517, 2520, 2524, 2533, 2534, 2537, 2544, 2545, 2548, 2550, 2552, 2559, 2569, 2570, 2573, 2591, 2598, 2604, 2617, 2620, 2621, 2624, 2626, 2628, 2630, 2631, 2636, 2640, 2641, 2654, 2658, 2666, 2668, 2671, 2682, 2684, 2700, 2702, 2703, 2705, 2707, 2709, 2713, 2715, 2720, 2729, 2751, 2756, 2759, 2767, 2772, 2778, 2781, 2782, 2783, 2786, 2791, 2793, 2795, 2796, 2797, 2804, 2805, 2810, 2817, 2818, 2833, 2841, 2844, 2847, 2850, 2854, 2856, 2861, 2865, 2869, 2876, 2878, 2882, 2887, 2888, 2890, 2895, 2896, 2897, 2899, 2901, 2918, 2919, 2921, 2924, 2935, 2939, 2943, 2955, 2956, 2957, 2960, 2961, 2963, 2975, 2976, 2980, 2996, 2999, 3000, 3002, 3003, 3014, 3017, 3022, 3025, 3029, 3032, 3045, 3046, 3047, 3049, 3050, 3051, 3056, 3058, 3060, 3067]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.4.output.dense\n",
      "  Dead Neurons: [3, 4, 10, 11, 13, 14, 15, 16, 17, 19, 24, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 82, 85, 87, 90, 93, 95, 97, 100, 105, 110, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 180, 185, 187, 193, 202, 204, 206, 211, 213, 214, 215, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 330, 331, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 468, 491, 492, 498, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 718, 720, 728, 731, 733, 735, 742, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.query\n",
      "  Dead Neurons: [32, 35, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 126, 127, 448, 449, 453, 454, 455, 459, 460, 463, 465, 467, 474, 477, 478, 484, 485, 490, 492, 493, 497, 500, 503, 506, 507, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 574, 575, 578, 583, 584, 585, 600, 609, 611, 623, 636, 667]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.key\n",
      "  Dead Neurons: [43, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 99, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 116, 118, 120, 121, 122, 123, 126, 127, 141, 147, 345, 448, 449, 450, 452, 453, 454, 455, 457, 458, 459, 460, 461, 463, 464, 466, 468, 469, 470, 471, 474, 475, 476, 477, 478, 479, 484, 485, 486, 487, 489, 490, 492, 493, 497, 499, 500, 501, 503, 506, 507, 510, 512, 513, 514, 515, 516, 518, 520, 523, 524, 525, 527, 528, 532, 533, 534, 537, 538, 539, 540, 541, 542, 544, 549, 551, 554, 555, 556, 557, 558, 560, 562, 563, 564, 565, 566, 568, 569, 570, 571, 574, 575, 578, 583, 585, 591, 592, 595, 608, 609, 611, 612, 617, 622, 623, 624, 626, 632, 634, 636, 667, 735]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.self.value\n",
      "  Dead Neurons: [67, 71, 76, 83, 99, 113, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 260, 265, 269, 273, 277, 279, 285, 286, 293, 294, 295, 297, 300, 302, 310, 311, 314, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 427, 442, 443]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 17, 19, 24, 29, 34, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 85, 87, 90, 93, 95, 100, 105, 110, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 204, 206, 211, 214, 215, 218, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 300, 302, 306, 324, 326, 327, 333, 342, 359, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 453, 468, 486, 491, 492, 494, 498, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 663, 664, 672, 676, 689, 718, 720, 728, 733, 735, 741, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.intermediate.dense\n",
      "  Dead Neurons: [2, 3, 4, 16, 18, 19, 24, 29, 34, 43, 44, 48, 54, 56, 57, 62, 72, 73, 82, 101, 124, 130, 133, 142, 144, 150, 151, 155, 167, 168, 177, 179, 183, 184, 185, 187, 194, 195, 196, 202, 205, 210, 211, 218, 229, 235, 237, 242, 253, 261, 268, 269, 273, 276, 290, 295, 305, 306, 311, 313, 332, 336, 338, 345, 351, 356, 358, 370, 372, 378, 382, 387, 400, 407, 418, 422, 424, 428, 432, 434, 438, 442, 444, 455, 458, 462, 468, 472, 476, 488, 491, 494, 507, 508, 516, 520, 524, 531, 534, 535, 539, 543, 548, 554, 560, 564, 568, 569, 579, 583, 587, 590, 603, 606, 609, 612, 613, 620, 622, 623, 626, 628, 631, 634, 638, 639, 646, 650, 677, 681, 685, 686, 701, 702, 705, 707, 710, 713, 718, 719, 722, 726, 727, 733, 740, 745, 746, 748, 753, 760, 761, 767, 774, 775, 780, 782, 783, 784, 787, 799, 800, 808, 810, 814, 822, 824, 827, 831, 833, 840, 842, 850, 856, 858, 861, 862, 867, 869, 879, 881, 883, 886, 892, 893, 898, 902, 904, 918, 919, 920, 925, 926, 927, 930, 936, 943, 944, 949, 952, 956, 958, 959, 960, 966, 968, 971, 979, 980, 982, 984, 990, 991, 1000, 1005, 1008, 1010, 1011, 1022, 1034, 1035, 1050, 1060, 1064, 1070, 1071, 1081, 1084, 1092, 1094, 1095, 1099, 1102, 1112, 1113, 1118, 1122, 1128, 1131, 1133, 1134, 1135, 1139, 1142, 1144, 1148, 1151, 1155, 1158, 1159, 1160, 1162, 1167, 1168, 1169, 1185, 1192, 1195, 1199, 1208, 1222, 1223, 1237, 1240, 1241, 1265, 1267, 1271, 1279, 1280, 1289, 1291, 1295, 1296, 1298, 1300, 1305, 1307, 1316, 1319, 1327, 1331, 1340, 1343, 1349, 1367, 1369, 1373, 1374, 1382, 1391, 1397, 1398, 1404, 1413, 1418, 1429, 1433, 1442, 1443, 1448, 1449, 1456, 1462, 1463, 1467, 1471, 1474, 1481, 1493, 1494, 1496, 1497, 1498, 1505, 1507, 1510, 1523, 1525, 1531, 1538, 1543, 1545, 1553, 1562, 1569, 1577, 1579, 1580, 1594, 1595, 1598, 1600, 1601, 1602, 1605, 1607, 1628, 1629, 1633, 1654, 1655, 1659, 1677, 1680, 1681, 1685, 1687, 1690, 1693, 1704, 1706, 1708, 1727, 1728, 1732, 1743, 1748, 1755, 1767, 1769, 1770, 1780, 1790, 1795, 1798, 1801, 1807, 1810, 1813, 1819, 1820, 1827, 1833, 1845, 1850, 1852, 1855, 1863, 1872, 1879, 1903, 1906, 1917, 1919, 1921, 1938, 1947, 1950, 1954, 1957, 1961, 1963, 1977, 1980, 1982, 1983, 1991, 2000, 2002, 2006, 2011, 2013, 2015, 2017, 2018, 2021, 2024, 2033, 2034, 2042, 2046, 2053, 2060, 2062, 2063, 2067, 2069, 2081, 2089, 2094, 2102, 2103, 2119, 2121, 2131, 2135, 2140, 2143, 2145, 2148, 2161, 2163, 2172, 2176, 2178, 2194, 2195, 2205, 2206, 2215, 2217, 2222, 2231, 2232, 2234, 2235, 2238, 2240, 2241, 2243, 2248, 2249, 2250, 2261, 2262, 2267, 2270, 2273, 2285, 2290, 2292, 2303, 2304, 2306, 2308, 2316, 2321, 2323, 2325, 2333, 2335, 2345, 2346, 2359, 2365, 2368, 2369, 2370, 2386, 2396, 2397, 2398, 2408, 2409, 2416, 2418, 2421, 2431, 2437, 2438, 2445, 2451, 2452, 2466, 2468, 2475, 2476, 2482, 2492, 2518, 2523, 2524, 2526, 2527, 2528, 2530, 2533, 2534, 2535, 2538, 2546, 2552, 2564, 2577, 2579, 2584, 2596, 2603, 2614, 2622, 2625, 2628, 2631, 2634, 2636, 2643, 2645, 2651, 2655, 2660, 2681, 2688, 2689, 2702, 2705, 2706, 2709, 2716, 2718, 2720, 2722, 2730, 2739, 2742, 2746, 2754, 2764, 2765, 2774, 2785, 2787, 2798, 2801, 2802, 2803, 2806, 2822, 2825, 2827, 2832, 2833, 2842, 2846, 2849, 2851, 2854, 2859, 2864, 2867, 2870, 2871, 2872, 2874, 2875, 2877, 2893, 2894, 2900, 2901, 2903, 2909, 2912, 2914, 2915, 2932, 2934, 2937, 2945, 2946, 2962, 2968, 2972, 2977, 2978, 2979, 2989, 2990, 2992, 3001, 3004, 3006, 3011, 3012, 3022, 3028, 3034, 3035, 3042, 3048, 3049, 3058, 3061, 3065, 3067]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.5.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 17, 19, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 85, 87, 90, 93, 95, 97, 100, 105, 110, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 198, 202, 204, 206, 211, 213, 214, 215, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 331, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 468, 491, 492, 497, 498, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 718, 720, 728, 731, 733, 735, 742, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.query\n",
      "  Dead Neurons: [194, 195, 196, 199, 205, 206, 207, 209, 210, 211, 214, 216, 218, 219, 223, 224, 228, 231, 232, 233, 239, 240, 241, 250, 252, 253, 254, 257, 258, 259, 260, 262, 264, 269, 270, 271, 272, 276, 277, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 295, 296, 301, 302, 304, 308, 310, 312, 313, 314, 316, 317, 318, 328, 331, 349, 378, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 440, 441, 442, 444, 445, 446, 447, 452, 453, 454, 455, 456, 457, 460, 462, 464, 474, 475, 477, 478, 479, 481, 483, 486, 487, 488, 493, 494, 497, 503, 506, 510, 644, 649, 673, 674, 692, 734]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.key\n",
      "  Dead Neurons: [109, 186, 195, 199, 200, 203, 205, 208, 209, 211, 216, 218, 219, 222, 224, 228, 231, 232, 233, 235, 236, 238, 239, 240, 241, 244, 247, 249, 252, 253, 254, 269, 271, 280, 281, 285, 288, 291, 293, 296, 300, 301, 302, 304, 310, 313, 317, 328, 331, 349, 378, 384, 385, 386, 388, 389, 390, 391, 392, 393, 394, 397, 398, 400, 401, 402, 404, 405, 406, 407, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 440, 441, 442, 444, 445, 446, 447, 451, 452, 453, 454, 456, 457, 461, 464, 468, 469, 473, 474, 475, 477, 478, 481, 482, 483, 485, 486, 487, 488, 493, 494, 495, 497, 498, 499, 501, 506, 508, 509, 511, 640, 644, 649, 655, 661, 667, 668, 669, 673, 674, 679, 682, 696, 697, 728, 734, 765, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.self.value\n",
      "  Dead Neurons: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 141, 146, 147, 148, 149, 150, 151, 155, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 183, 184, 186, 190, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 264, 267, 269, 273, 275, 276, 283, 284, 287, 288, 289, 295, 296, 302, 303, 304, 309, 310, 313, 315, 318, 526, 580, 584, 586, 588, 590, 596, 597, 599, 601, 604, 605, 608, 609, 613, 615, 620, 623, 625, 628, 629, 631, 632, 634, 636, 637, 638, 639]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.attention.output.dense\n",
      "  Dead Neurons: [3, 4, 10, 11, 13, 14, 15, 16, 19, 29, 38, 41, 46, 47, 57, 61, 63, 67, 73, 75, 77, 82, 85, 87, 90, 93, 95, 97, 100, 105, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 206, 211, 214, 215, 217, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 287, 297, 300, 302, 306, 324, 327, 330, 331, 333, 340, 342, 355, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 453, 468, 491, 492, 494, 496, 498, 518, 519, 529, 538, 540, 546, 551, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 609, 611, 624, 634, 641, 647, 652, 653, 655, 656, 662, 663, 664, 672, 676, 689, 699, 718, 720, 728, 731, 733, 735, 745, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.intermediate.dense\n",
      "  Dead Neurons: [0, 2, 12, 13, 22, 28, 33, 41, 53, 54, 62, 68, 73, 75, 83, 84, 99, 103, 104, 112, 122, 132, 143, 151, 157, 161, 162, 163, 164, 166, 174, 176, 181, 182, 184, 198, 202, 204, 207, 211, 217, 222, 233, 234, 239, 242, 251, 260, 261, 262, 265, 270, 271, 272, 273, 280, 293, 303, 309, 327, 330, 331, 334, 338, 341, 346, 348, 349, 350, 355, 367, 369, 374, 376, 377, 379, 389, 390, 419, 426, 427, 428, 431, 432, 436, 446, 447, 448, 455, 456, 458, 467, 478, 483, 499, 503, 505, 511, 513, 517, 525, 526, 527, 533, 539, 540, 541, 549, 569, 572, 577, 579, 588, 601, 610, 613, 616, 619, 623, 626, 627, 628, 630, 631, 635, 639, 643, 649, 662, 663, 683, 684, 690, 695, 698, 699, 700, 701, 706, 708, 716, 718, 728, 729, 731, 751, 754, 759, 760, 766, 778, 782, 787, 789, 793, 804, 810, 815, 816, 828, 830, 832, 833, 837, 838, 843, 867, 871, 874, 875, 881, 882, 884, 896, 899, 900, 905, 911, 915, 920, 928, 931, 932, 938, 939, 940, 942, 945, 953, 957, 959, 960, 963, 966, 972, 979, 986, 994, 995, 998, 1000, 1007, 1008, 1012, 1015, 1026, 1035, 1037, 1040, 1046, 1047, 1053, 1059, 1068, 1074, 1089, 1094, 1095, 1096, 1100, 1105, 1107, 1122, 1127, 1132, 1134, 1135, 1140, 1141, 1142, 1150, 1152, 1153, 1164, 1165, 1166, 1167, 1171, 1173, 1174, 1178, 1186, 1190, 1192, 1193, 1196, 1201, 1203, 1225, 1228, 1229, 1236, 1241, 1259, 1261, 1263, 1271, 1277, 1278, 1281, 1287, 1291, 1294, 1295, 1297, 1300, 1310, 1314, 1315, 1322, 1357, 1358, 1363, 1367, 1371, 1378, 1387, 1390, 1400, 1402, 1407, 1408, 1409, 1412, 1416, 1421, 1437, 1450, 1453, 1457, 1461, 1464, 1466, 1470, 1473, 1478, 1487, 1498, 1499, 1502, 1510, 1516, 1520, 1525, 1528, 1530, 1533, 1535, 1536, 1537, 1540, 1543, 1544, 1546, 1548, 1566, 1570, 1573, 1576, 1579, 1584, 1592, 1595, 1597, 1600, 1614, 1615, 1617, 1624, 1625, 1627, 1629, 1631, 1645, 1647, 1650, 1656, 1661, 1668, 1677, 1678, 1689, 1690, 1692, 1699, 1707, 1715, 1716, 1723, 1724, 1725, 1726, 1729, 1733, 1737, 1740, 1746, 1754, 1764, 1766, 1774, 1777, 1782, 1793, 1795, 1804, 1805, 1808, 1810, 1820, 1827, 1835, 1838, 1841, 1844, 1846, 1847, 1854, 1855, 1864, 1873, 1890, 1891, 1897, 1899, 1905, 1910, 1911, 1912, 1914, 1915, 1919, 1923, 1926, 1929, 1939, 1940, 1942, 1948, 1953, 1955, 1960, 1966, 1972, 1973, 1976, 1982, 1988, 1990, 2003, 2007, 2014, 2026, 2029, 2039, 2040, 2046, 2051, 2053, 2055, 2060, 2068, 2069, 2072, 2077, 2080, 2085, 2094, 2098, 2103, 2105, 2107, 2119, 2125, 2136, 2138, 2146, 2148, 2159, 2162, 2172, 2173, 2175, 2177, 2178, 2180, 2181, 2182, 2183, 2191, 2197, 2200, 2207, 2218, 2219, 2225, 2226, 2234, 2235, 2239, 2250, 2261, 2269, 2274, 2290, 2297, 2299, 2302, 2303, 2308, 2313, 2323, 2332, 2341, 2360, 2362, 2367, 2370, 2371, 2383, 2385, 2393, 2395, 2421, 2423, 2433, 2435, 2445, 2449, 2451, 2452, 2458, 2460, 2461, 2465, 2467, 2472, 2476, 2477, 2479, 2480, 2482, 2484, 2500, 2502, 2507, 2529, 2534, 2538, 2539, 2542, 2546, 2555, 2564, 2568, 2572, 2574, 2576, 2577, 2586, 2588, 2593, 2599, 2601, 2602, 2609, 2610, 2620, 2624, 2627, 2631, 2634, 2636, 2644, 2650, 2653, 2655, 2663, 2668, 2670, 2676, 2683, 2686, 2689, 2693, 2694, 2707, 2709, 2710, 2712, 2715, 2716, 2724, 2730, 2744, 2757, 2764, 2767, 2774, 2779, 2782, 2787, 2794, 2795, 2797, 2812, 2842, 2844, 2847, 2848, 2850, 2853, 2857, 2859, 2883, 2889, 2896, 2899, 2914, 2921, 2926, 2927, 2931, 2936, 2937, 2945, 2946, 2951, 2952, 2955, 2959, 2963, 2964, 2965, 2972, 2983, 2991, 3000, 3006, 3012, 3017, 3022, 3024, 3037, 3040, 3042, 3045, 3053, 3057, 3059, 3061, 3063, 3064, 3070]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.6.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 19, 24, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 85, 87, 90, 93, 95, 97, 100, 105, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 188, 193, 195, 198, 202, 206, 211, 213, 214, 215, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 297, 302, 306, 324, 326, 327, 333, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 468, 491, 492, 497, 498, 518, 519, 529, 538, 540, 541, 546, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 689, 694, 718, 720, 731, 733, 735, 742, 745, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.query\n",
      "  Dead Neurons: [19, 66, 69, 76, 82, 102, 122, 123, 126, 141, 146, 187, 192, 193, 194, 195, 196, 197, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 215, 219, 222, 224, 226, 228, 230, 232, 233, 235, 239, 240, 241, 243, 244, 245, 247, 248, 249, 251, 252, 254, 255, 266, 280, 303, 307, 320, 323, 331, 332, 334, 335, 336, 338, 339, 341, 342, 343, 346, 350, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 370, 371, 373, 374, 376, 379, 381, 382, 383, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 721]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.key\n",
      "  Dead Neurons: [19, 66, 109, 114, 122, 133, 141, 146, 149, 163, 165, 166, 169, 170, 179, 181, 187, 192, 193, 194, 195, 197, 200, 202, 203, 205, 206, 207, 208, 209, 210, 211, 214, 215, 217, 219, 222, 224, 226, 228, 230, 232, 233, 234, 235, 236, 239, 240, 243, 244, 245, 247, 248, 249, 252, 255, 320, 323, 324, 326, 327, 329, 330, 331, 332, 333, 334, 336, 338, 340, 342, 348, 349, 351, 352, 357, 358, 359, 360, 362, 363, 366, 367, 369, 370, 374, 375, 377, 378, 379, 380, 504, 594, 635, 640, 641, 642, 644, 645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 696, 697, 698, 699, 700, 707, 721, 760]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.self.value\n",
      "  Dead Neurons: [3, 9, 13, 19, 21, 28, 37, 39, 42, 44, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 457, 484, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 707, 710, 713, 714, 716, 719, 721, 725, 728, 734, 735, 738, 752, 753, 766]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.attention.output.dense\n",
      "  Dead Neurons: [3, 4, 10, 11, 13, 14, 15, 16, 19, 24, 29, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 77, 82, 85, 87, 90, 93, 95, 100, 105, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 193, 202, 206, 211, 214, 215, 218, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 330, 331, 333, 342, 359, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 453, 468, 491, 492, 496, 497, 498, 518, 529, 538, 540, 546, 551, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 609, 611, 623, 624, 634, 641, 647, 652, 653, 655, 662, 663, 664, 672, 676, 689, 694, 699, 718, 720, 728, 731, 733, 735, 741, 742, 745, 748, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.intermediate.dense\n",
      "  Dead Neurons: [3, 7, 9, 18, 20, 23, 24, 27, 41, 68, 72, 86, 100, 124, 126, 128, 129, 133, 147, 148, 153, 155, 163, 166, 169, 173, 178, 180, 184, 186, 190, 194, 203, 206, 221, 222, 234, 241, 250, 259, 261, 264, 268, 273, 276, 277, 279, 282, 285, 297, 305, 309, 312, 315, 321, 323, 325, 330, 331, 344, 346, 348, 349, 350, 353, 358, 364, 367, 384, 386, 388, 391, 395, 396, 401, 405, 407, 408, 417, 424, 425, 435, 439, 445, 449, 455, 467, 469, 485, 487, 490, 492, 507, 511, 515, 520, 521, 522, 523, 528, 529, 536, 543, 544, 554, 558, 566, 570, 573, 577, 582, 584, 588, 598, 602, 610, 612, 624, 630, 636, 644, 645, 649, 650, 659, 674, 675, 687, 691, 695, 697, 698, 711, 719, 721, 722, 723, 725, 728, 741, 745, 752, 755, 763, 767, 778, 782, 783, 790, 801, 803, 806, 808, 809, 810, 813, 818, 821, 823, 826, 829, 831, 835, 837, 842, 844, 847, 853, 857, 863, 869, 871, 873, 886, 888, 890, 891, 896, 905, 910, 916, 917, 918, 925, 929, 930, 931, 938, 941, 945, 951, 959, 969, 982, 990, 991, 1022, 1024, 1031, 1032, 1034, 1048, 1054, 1057, 1064, 1069, 1070, 1073, 1078, 1085, 1087, 1090, 1095, 1097, 1106, 1124, 1129, 1131, 1135, 1137, 1149, 1151, 1161, 1162, 1163, 1167, 1191, 1195, 1201, 1205, 1213, 1214, 1217, 1224, 1247, 1250, 1254, 1257, 1266, 1268, 1269, 1272, 1279, 1288, 1291, 1292, 1310, 1311, 1317, 1321, 1324, 1335, 1338, 1347, 1353, 1357, 1361, 1362, 1369, 1376, 1379, 1381, 1382, 1386, 1398, 1399, 1400, 1418, 1420, 1427, 1444, 1448, 1449, 1451, 1452, 1454, 1456, 1459, 1463, 1466, 1469, 1471, 1474, 1486, 1495, 1506, 1507, 1508, 1520, 1524, 1526, 1528, 1554, 1555, 1558, 1561, 1564, 1572, 1582, 1586, 1589, 1596, 1601, 1612, 1614, 1630, 1631, 1634, 1639, 1657, 1661, 1663, 1669, 1672, 1673, 1674, 1678, 1680, 1686, 1689, 1694, 1696, 1697, 1699, 1706, 1714, 1721, 1724, 1728, 1732, 1733, 1752, 1754, 1755, 1762, 1768, 1787, 1788, 1789, 1793, 1798, 1799, 1801, 1806, 1808, 1809, 1813, 1814, 1821, 1827, 1832, 1834, 1846, 1848, 1856, 1859, 1864, 1866, 1875, 1876, 1880, 1883, 1890, 1891, 1893, 1894, 1898, 1900, 1905, 1906, 1907, 1909, 1913, 1919, 1920, 1926, 1930, 1941, 1942, 1945, 1946, 1949, 1959, 1964, 1966, 1969, 1977, 1987, 1992, 1993, 1994, 2008, 2010, 2016, 2025, 2027, 2029, 2032, 2038, 2040, 2046, 2066, 2071, 2072, 2073, 2079, 2085, 2087, 2104, 2105, 2107, 2108, 2109, 2121, 2122, 2123, 2124, 2127, 2128, 2135, 2136, 2138, 2139, 2146, 2151, 2154, 2162, 2167, 2172, 2173, 2176, 2178, 2183, 2192, 2200, 2202, 2206, 2209, 2211, 2216, 2223, 2227, 2229, 2232, 2237, 2253, 2258, 2273, 2279, 2280, 2284, 2290, 2291, 2299, 2301, 2308, 2314, 2317, 2318, 2322, 2327, 2328, 2329, 2330, 2332, 2340, 2341, 2347, 2350, 2353, 2361, 2362, 2367, 2371, 2375, 2386, 2390, 2397, 2410, 2414, 2424, 2431, 2440, 2444, 2450, 2451, 2457, 2463, 2470, 2481, 2489, 2493, 2494, 2503, 2504, 2506, 2507, 2512, 2525, 2530, 2533, 2536, 2538, 2540, 2550, 2551, 2552, 2557, 2560, 2569, 2579, 2580, 2581, 2588, 2590, 2603, 2619, 2622, 2623, 2628, 2629, 2633, 2640, 2649, 2650, 2653, 2654, 2659, 2663, 2666, 2671, 2672, 2677, 2684, 2692, 2698, 2699, 2703, 2709, 2719, 2720, 2721, 2723, 2728, 2732, 2741, 2742, 2753, 2754, 2758, 2774, 2775, 2776, 2783, 2790, 2796, 2802, 2807, 2815, 2816, 2818, 2819, 2820, 2821, 2826, 2828, 2829, 2831, 2832, 2834, 2835, 2839, 2840, 2842, 2845, 2852, 2855, 2858, 2860, 2862, 2863, 2871, 2872, 2902, 2908, 2909, 2914, 2917, 2919, 2926, 2929, 2930, 2933, 2941, 2950, 2959, 2966, 2970, 2975, 2976, 2996, 2998, 3000, 3008, 3009, 3010, 3015, 3016, 3032, 3036, 3049, 3063, 3065, 3069]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.7.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 19, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 83, 85, 87, 90, 93, 95, 97, 100, 105, 111, 115, 119, 122, 124, 125, 131, 133, 144, 157, 159, 161, 169, 185, 187, 188, 193, 195, 198, 202, 204, 206, 211, 213, 214, 215, 217, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 251, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 333, 359, 361, 366, 373, 376, 378, 387, 388, 397, 403, 405, 408, 409, 416, 417, 428, 432, 452, 453, 468, 491, 492, 498, 511, 518, 519, 529, 538, 540, 546, 559, 561, 564, 570, 573, 577, 578, 580, 583, 586, 604, 609, 611, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 684, 689, 694, 695, 699, 718, 720, 731, 733, 735, 742, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.query\n",
      "  Dead Neurons: [1, 36, 53, 54, 64, 67, 70, 74, 85, 86, 92, 102, 118, 119, 124, 192, 193, 197, 200, 202, 203, 205, 209, 212, 214, 215, 226, 231, 234, 238, 239, 241, 242, 244, 251, 253, 254, 265, 281, 289, 292, 298, 299, 306, 325, 340, 375, 455, 483, 493, 500, 585, 640, 641, 642, 644, 645, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 662, 663, 664, 665, 666, 667, 669, 670, 671, 672, 673, 674, 675, 676, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 708, 709, 710, 713, 715, 716, 718, 720, 723, 724, 726, 727, 728, 730, 732, 733, 735, 736, 739, 740, 741, 742, 743, 744, 746, 747, 748, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 761, 763, 764, 765, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.key\n",
      "  Dead Neurons: [1, 12, 30, 53, 74, 75, 86, 110, 118, 119, 128, 192, 197, 204, 205, 207, 213, 216, 235, 238, 239, 242, 247, 268, 281, 283, 298, 355, 357, 409, 422, 483, 500, 506, 507, 562, 579, 581, 585, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 697, 698, 699, 701, 702, 703, 704, 705, 706, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 721, 722, 723, 724, 725, 726, 727, 728, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 743, 744, 745, 747, 748, 749, 750, 752, 754, 755, 756, 757, 758, 759, 760, 762, 763, 764, 765, 766, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.self.value\n",
      "  Dead Neurons: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 141, 392, 431, 436, 440, 447, 485, 487, 495, 497, 503, 507, 508, 510, 511, 513, 515, 517, 520, 521, 529, 531, 534, 535, 536, 541, 544, 546, 552, 553, 556, 560, 563, 568, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.attention.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 14, 15, 16, 19, 24, 29, 38, 40, 41, 46, 47, 55, 57, 61, 63, 67, 73, 75, 77, 79, 82, 85, 87, 90, 93, 97, 100, 105, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 161, 169, 176, 180, 185, 187, 188, 193, 202, 204, 206, 211, 214, 215, 217, 218, 219, 224, 225, 235, 240, 245, 247, 254, 259, 265, 278, 282, 287, 302, 306, 324, 326, 327, 330, 331, 333, 342, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 409, 416, 417, 428, 432, 453, 468, 492, 496, 497, 498, 518, 519, 529, 538, 540, 546, 551, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 609, 611, 624, 634, 641, 647, 652, 653, 655, 656, 662, 663, 664, 672, 676, 689, 699, 718, 720, 728, 731, 733, 735, 741, 745, 748, 749, 750, 751, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.intermediate.dense\n",
      "  Dead Neurons: [1, 12, 26, 30, 31, 34, 44, 52, 60, 68, 71, 77, 79, 87, 92, 93, 100, 101, 122, 123, 126, 133, 155, 160, 161, 166, 168, 171, 178, 180, 181, 195, 203, 209, 212, 216, 220, 224, 229, 230, 231, 235, 239, 246, 249, 254, 256, 260, 267, 269, 278, 284, 290, 293, 295, 312, 323, 328, 334, 335, 337, 350, 351, 357, 358, 359, 361, 367, 368, 370, 381, 388, 398, 401, 406, 407, 421, 427, 446, 451, 455, 459, 461, 467, 481, 491, 495, 498, 499, 504, 505, 506, 509, 514, 516, 518, 519, 524, 526, 527, 533, 534, 542, 546, 548, 549, 554, 555, 560, 570, 577, 582, 585, 589, 594, 597, 598, 603, 609, 610, 613, 615, 630, 633, 636, 651, 658, 659, 668, 671, 673, 678, 679, 688, 692, 697, 698, 702, 704, 706, 710, 715, 716, 717, 719, 721, 723, 726, 732, 738, 753, 754, 758, 759, 761, 764, 766, 771, 774, 777, 786, 788, 789, 798, 799, 807, 810, 828, 829, 830, 839, 841, 859, 869, 882, 885, 886, 888, 892, 897, 902, 911, 913, 914, 915, 916, 930, 932, 938, 942, 954, 956, 958, 968, 970, 971, 979, 980, 984, 993, 1007, 1016, 1019, 1021, 1027, 1028, 1035, 1048, 1049, 1062, 1073, 1080, 1090, 1095, 1101, 1105, 1109, 1115, 1117, 1123, 1126, 1127, 1128, 1130, 1137, 1139, 1141, 1144, 1148, 1149, 1150, 1155, 1160, 1164, 1173, 1174, 1179, 1183, 1184, 1192, 1194, 1195, 1206, 1220, 1238, 1240, 1247, 1248, 1254, 1255, 1256, 1258, 1262, 1267, 1269, 1281, 1292, 1293, 1301, 1308, 1316, 1319, 1321, 1328, 1330, 1338, 1344, 1360, 1361, 1363, 1364, 1372, 1375, 1378, 1383, 1389, 1403, 1410, 1427, 1432, 1456, 1461, 1462, 1482, 1487, 1492, 1495, 1497, 1506, 1509, 1518, 1521, 1524, 1533, 1536, 1538, 1540, 1542, 1548, 1559, 1566, 1567, 1583, 1584, 1585, 1586, 1589, 1590, 1596, 1598, 1600, 1601, 1602, 1607, 1618, 1619, 1625, 1626, 1635, 1638, 1643, 1644, 1659, 1661, 1663, 1664, 1670, 1671, 1673, 1677, 1682, 1688, 1689, 1702, 1706, 1707, 1710, 1717, 1718, 1720, 1724, 1726, 1728, 1749, 1754, 1755, 1766, 1768, 1776, 1780, 1784, 1787, 1790, 1793, 1803, 1804, 1807, 1817, 1818, 1820, 1825, 1835, 1838, 1845, 1854, 1860, 1863, 1877, 1878, 1879, 1881, 1883, 1899, 1903, 1913, 1921, 1924, 1926, 1928, 1935, 1952, 1957, 1959, 1966, 1971, 1974, 1980, 1983, 1984, 1985, 1987, 1989, 1991, 1996, 2003, 2006, 2011, 2012, 2015, 2018, 2022, 2026, 2032, 2034, 2039, 2040, 2043, 2047, 2052, 2053, 2058, 2059, 2062, 2078, 2080, 2083, 2094, 2098, 2102, 2109, 2110, 2114, 2118, 2122, 2127, 2128, 2129, 2138, 2146, 2148, 2150, 2151, 2162, 2172, 2183, 2189, 2202, 2205, 2206, 2211, 2212, 2214, 2233, 2244, 2246, 2249, 2259, 2264, 2272, 2282, 2287, 2291, 2294, 2296, 2299, 2307, 2308, 2311, 2316, 2317, 2318, 2326, 2333, 2340, 2344, 2345, 2346, 2350, 2351, 2367, 2373, 2374, 2377, 2383, 2385, 2387, 2388, 2390, 2398, 2404, 2405, 2407, 2408, 2409, 2413, 2424, 2428, 2435, 2437, 2443, 2448, 2465, 2468, 2480, 2481, 2482, 2489, 2499, 2501, 2502, 2505, 2506, 2507, 2509, 2510, 2511, 2517, 2529, 2537, 2538, 2550, 2554, 2563, 2567, 2576, 2579, 2580, 2586, 2588, 2596, 2600, 2603, 2611, 2618, 2628, 2629, 2630, 2634, 2643, 2648, 2649, 2650, 2657, 2658, 2660, 2665, 2670, 2671, 2672, 2676, 2679, 2680, 2683, 2684, 2698, 2700, 2701, 2705, 2707, 2712, 2730, 2735, 2756, 2759, 2762, 2770, 2776, 2779, 2787, 2794, 2795, 2809, 2814, 2816, 2819, 2821, 2824, 2827, 2828, 2829, 2830, 2857, 2861, 2862, 2869, 2873, 2876, 2879, 2880, 2888, 2889, 2891, 2893, 2910, 2912, 2927, 2930, 2932, 2941, 2957, 2961, 2962, 2963, 2965, 2966, 2982, 2985, 2990, 2992, 2993, 2994, 3000, 3001, 3002, 3011, 3017, 3023, 3031, 3035, 3036, 3049, 3053, 3055, 3060]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.8.output.dense\n",
      "  Dead Neurons: [4, 10, 11, 13, 15, 16, 19, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 77, 82, 85, 87, 90, 93, 95, 97, 100, 102, 105, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 157, 159, 161, 169, 185, 187, 188, 193, 195, 198, 202, 204, 206, 211, 213, 214, 217, 218, 219, 224, 225, 231, 232, 235, 240, 245, 247, 254, 259, 265, 278, 282, 286, 287, 302, 306, 324, 326, 327, 330, 331, 333, 359, 361, 366, 373, 376, 378, 388, 397, 403, 405, 408, 409, 417, 428, 432, 446, 452, 453, 468, 491, 492, 496, 498, 511, 518, 519, 529, 538, 540, 551, 559, 561, 564, 570, 573, 577, 578, 583, 586, 604, 609, 611, 624, 634, 641, 647, 652, 653, 655, 656, 662, 664, 672, 676, 684, 689, 695, 699, 718, 720, 731, 733, 735, 748, 749, 750, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.query\n",
      "  Dead Neurons: [24, 49, 92, 128, 129, 130, 131, 132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 210, 257, 277, 278, 280, 282, 288, 293, 297, 300, 308, 318, 321, 324, 329, 331, 332, 335, 340, 341, 347, 349, 351, 355, 359, 362, 363, 367, 370, 372, 375, 381, 385, 436, 449, 450, 458, 468, 471, 483, 484, 489, 490, 491, 493, 498, 503, 523, 559, 571, 689, 699, 702, 704, 708, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 728, 729, 731, 732, 734, 735, 736, 740, 741, 743, 744, 747, 748, 749, 750, 751, 752, 753, 754, 757, 759, 760, 764, 765, 766]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.key\n",
      "  Dead Neurons: [24, 36, 49, 57, 80, 92, 108, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 208, 271, 276, 280, 283, 287, 292, 293, 299, 303, 306, 307, 318, 319, 320, 324, 325, 327, 331, 335, 337, 340, 345, 349, 353, 363, 367, 368, 370, 372, 373, 374, 376, 381, 382, 411, 420, 494, 507, 512, 559, 564, 604, 669, 695, 699, 704, 705, 706, 707, 708, 709, 712, 713, 715, 716, 717, 718, 719, 720, 721, 723, 724, 725, 727, 731, 732, 733, 734, 735, 736, 737, 741, 742, 744, 745, 747, 748, 749, 750, 752, 754, 757, 759, 760, 764, 765, 766]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.self.value\n",
      "  Dead Neurons: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 212, 217, 218, 223, 224, 229, 230, 240, 246, 248, 250, 253, 254, 260, 261, 283, 298, 306, 386, 391, 395, 396, 406, 410, 412, 423, 429, 430, 438, 439, 443, 472, 531, 576, 577, 578, 579, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 632, 633, 634, 635, 636, 637, 638, 639, 689, 719]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.attention.output.dense\n",
      "  Dead Neurons: [4, 11, 13, 14, 15, 16, 19, 24, 29, 38, 40, 41, 46, 55, 57, 60, 61, 63, 67, 73, 75, 77, 79, 82, 87, 90, 93, 97, 100, 105, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 156, 157, 159, 169, 176, 180, 185, 187, 188, 202, 204, 206, 209, 211, 214, 215, 217, 218, 219, 225, 229, 232, 235, 240, 245, 247, 254, 259, 265, 278, 282, 286, 287, 300, 302, 324, 327, 330, 331, 333, 342, 359, 361, 366, 373, 376, 378, 387, 397, 403, 405, 408, 416, 417, 424, 428, 432, 446, 452, 453, 468, 492, 496, 497, 498, 511, 518, 519, 526, 529, 540, 541, 546, 551, 559, 561, 564, 570, 577, 578, 583, 586, 604, 606, 609, 611, 623, 624, 641, 647, 653, 655, 662, 663, 664, 672, 674, 676, 689, 699, 718, 720, 728, 731, 733, 735, 742, 745, 749, 750, 756, 762, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.intermediate.dense\n",
      "  Dead Neurons: [2, 13, 15, 19, 22, 24, 25, 31, 51, 54, 56, 58, 59, 61, 72, 83, 91, 92, 98, 108, 112, 114, 119, 123, 124, 126, 132, 138, 139, 140, 144, 147, 148, 149, 150, 153, 164, 168, 170, 173, 187, 189, 197, 199, 204, 207, 210, 214, 217, 219, 220, 224, 225, 228, 229, 240, 247, 250, 258, 271, 275, 296, 298, 307, 311, 314, 316, 317, 320, 328, 333, 338, 340, 347, 349, 352, 356, 363, 367, 369, 372, 377, 378, 380, 386, 394, 400, 404, 412, 413, 416, 419, 426, 436, 438, 442, 448, 455, 458, 463, 467, 474, 477, 478, 479, 486, 491, 497, 503, 512, 514, 515, 517, 519, 526, 533, 537, 541, 544, 547, 555, 558, 568, 577, 579, 587, 591, 599, 602, 606, 611, 612, 614, 616, 623, 624, 634, 635, 637, 643, 644, 645, 653, 654, 660, 664, 666, 669, 677, 679, 686, 700, 703, 710, 716, 719, 726, 731, 735, 742, 743, 745, 747, 748, 749, 763, 769, 773, 777, 790, 791, 792, 799, 814, 815, 825, 828, 832, 835, 836, 851, 855, 863, 864, 867, 869, 874, 875, 881, 882, 887, 894, 899, 903, 907, 910, 914, 916, 923, 924, 925, 927, 928, 929, 932, 933, 935, 937, 938, 949, 950, 961, 962, 966, 967, 976, 978, 980, 990, 995, 1000, 1015, 1016, 1019, 1021, 1022, 1029, 1049, 1052, 1053, 1054, 1057, 1059, 1064, 1067, 1068, 1069, 1077, 1082, 1083, 1084, 1092, 1093, 1094, 1100, 1101, 1109, 1115, 1127, 1128, 1133, 1142, 1143, 1144, 1145, 1149, 1151, 1153, 1154, 1155, 1157, 1158, 1160, 1163, 1165, 1172, 1174, 1179, 1186, 1192, 1199, 1202, 1206, 1209, 1212, 1216, 1229, 1235, 1239, 1243, 1244, 1259, 1264, 1269, 1285, 1286, 1293, 1298, 1299, 1302, 1306, 1307, 1308, 1310, 1314, 1317, 1325, 1328, 1331, 1339, 1345, 1355, 1358, 1362, 1363, 1371, 1373, 1374, 1377, 1378, 1389, 1398, 1399, 1400, 1408, 1411, 1416, 1418, 1421, 1425, 1427, 1435, 1439, 1449, 1467, 1468, 1476, 1480, 1487, 1489, 1492, 1498, 1499, 1501, 1502, 1512, 1513, 1517, 1519, 1521, 1522, 1528, 1534, 1543, 1546, 1547, 1548, 1549, 1553, 1557, 1559, 1575, 1588, 1589, 1590, 1591, 1592, 1596, 1600, 1607, 1610, 1619, 1621, 1626, 1630, 1631, 1632, 1636, 1652, 1653, 1654, 1656, 1668, 1673, 1675, 1678, 1693, 1699, 1700, 1710, 1711, 1712, 1720, 1725, 1728, 1729, 1735, 1741, 1743, 1760, 1764, 1772, 1774, 1776, 1777, 1779, 1782, 1796, 1819, 1822, 1829, 1838, 1839, 1859, 1861, 1862, 1863, 1865, 1877, 1878, 1880, 1882, 1900, 1921, 1927, 1932, 1956, 1958, 1968, 1973, 1980, 1981, 1992, 1995, 2000, 2004, 2005, 2010, 2011, 2015, 2016, 2019, 2023, 2025, 2030, 2033, 2043, 2051, 2057, 2058, 2075, 2076, 2079, 2080, 2083, 2088, 2092, 2095, 2099, 2106, 2110, 2111, 2125, 2133, 2135, 2140, 2145, 2153, 2161, 2163, 2164, 2170, 2173, 2174, 2181, 2182, 2187, 2190, 2191, 2193, 2201, 2203, 2206, 2226, 2231, 2232, 2233, 2235, 2236, 2247, 2252, 2259, 2264, 2268, 2272, 2274, 2287, 2290, 2296, 2301, 2304, 2308, 2315, 2322, 2335, 2347, 2356, 2358, 2373, 2382, 2386, 2395, 2397, 2400, 2404, 2412, 2414, 2423, 2425, 2426, 2427, 2429, 2431, 2436, 2438, 2440, 2441, 2445, 2448, 2467, 2470, 2473, 2474, 2480, 2481, 2487, 2490, 2496, 2502, 2504, 2509, 2516, 2518, 2522, 2533, 2541, 2545, 2551, 2553, 2558, 2566, 2578, 2591, 2596, 2597, 2608, 2616, 2618, 2620, 2624, 2626, 2627, 2640, 2654, 2657, 2669, 2670, 2680, 2685, 2693, 2694, 2699, 2702, 2711, 2721, 2724, 2725, 2736, 2746, 2750, 2756, 2768, 2772, 2774, 2784, 2786, 2802, 2807, 2817, 2818, 2821, 2825, 2829, 2840, 2847, 2868, 2877, 2883, 2887, 2893, 2907, 2921, 2922, 2931, 2939, 2971, 2972, 2974, 2979, 2980, 2984, 2991, 2999, 3002, 3009, 3013, 3016, 3038, 3041, 3048, 3054, 3055, 3057, 3066]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.9.output.dense\n",
      "  Dead Neurons: [3, 4, 5, 10, 11, 13, 15, 16, 19, 38, 40, 41, 46, 47, 55, 57, 60, 61, 63, 67, 73, 75, 77, 79, 82, 85, 87, 90, 93, 97, 100, 102, 105, 111, 115, 119, 122, 123, 124, 125, 131, 133, 144, 157, 159, 161, 169, 187, 193, 195, 198, 202, 204, 206, 211, 213, 214, 217, 219, 224, 225, 229, 231, 232, 235, 240, 245, 247, 254, 258, 259, 265, 278, 282, 286, 287, 297, 299, 300, 302, 306, 324, 326, 327, 330, 331, 333, 359, 361, 366, 373, 376, 378, 388, 397, 403, 405, 408, 409, 417, 424, 428, 432, 446, 452, 453, 468, 491, 496, 497, 498, 511, 518, 519, 538, 540, 551, 559, 561, 570, 573, 577, 578, 580, 583, 586, 587, 604, 611, 624, 634, 647, 652, 655, 656, 664, 672, 676, 684, 689, 695, 699, 718, 720, 731, 733, 735, 742, 749, 751, 752, 756, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.query\n",
      "  Dead Neurons: [11, 22, 24, 28, 30, 39, 43, 45, 49, 57, 65, 75, 79, 86, 93, 104, 109, 110, 111, 125, 129, 171, 256, 257, 258, 259, 260, 263, 264, 265, 269, 270, 271, 272, 274, 276, 277, 279, 280, 281, 282, 283, 286, 289, 290, 292, 294, 295, 296, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 387, 388, 391, 392, 394, 405, 410, 415, 423, 424, 428, 429, 430, 431, 433, 434, 441, 442, 447, 448, 451, 456, 457, 461, 463, 464, 467, 469, 479, 483, 487, 490, 494, 498, 501, 507, 516, 517, 518, 522, 526, 536, 537, 538, 539, 542, 544, 545, 546, 548, 557, 563, 564, 565, 566, 569, 573, 574, 642, 646, 648, 649, 651, 655, 657, 658, 659, 661, 662, 663, 667, 671, 674, 675, 676, 679, 682, 684, 685, 689, 692, 695, 699, 700, 702, 722]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.key\n",
      "  Dead Neurons: [11, 22, 26, 28, 37, 39, 45, 53, 73, 77, 79, 81, 85, 88, 93, 95, 100, 104, 106, 107, 109, 110, 114, 124, 125, 127, 159, 210, 211, 228, 256, 258, 259, 260, 261, 263, 265, 266, 268, 270, 271, 272, 274, 275, 276, 277, 279, 281, 282, 283, 285, 290, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 318, 319, 384, 386, 387, 389, 392, 393, 394, 396, 400, 405, 410, 411, 413, 414, 416, 420, 422, 423, 424, 426, 431, 434, 435, 437, 438, 441, 442, 445, 446, 457, 467, 470, 474, 475, 477, 490, 491, 492, 498, 501, 502, 511, 513, 517, 522, 533, 536, 537, 548, 557, 560, 564, 566, 567, 640, 645, 649, 652, 655, 657, 660, 663, 667, 671, 674, 675, 676, 677, 678, 679, 681, 682, 685, 692, 693, 696, 700, 710, 730, 766]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.self.value\n",
      "  Dead Neurons: [18, 27, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 142, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 174, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 224, 320, 322, 324, 325, 326, 328, 332, 333, 337, 338, 341, 345, 348, 350, 351, 354, 356, 359, 360, 366, 369, 370, 372, 377, 378, 379, 381, 576, 578, 579, 581, 582, 583, 584, 586, 587, 591, 592, 593, 594, 595, 596, 598, 599, 602, 603, 604, 606, 607, 608, 610, 612, 613, 615, 616, 617, 618, 619, 620, 622, 623, 626, 627, 628, 630, 633, 634, 636, 637, 638, 705, 706, 707, 709, 712, 713, 714, 719, 721, 724, 726, 727, 733, 734, 736, 738, 741, 742, 743, 744, 746, 747, 749, 752, 753, 754, 755, 758, 762, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.attention.output.dense\n",
      "  Dead Neurons: [4, 5, 11, 15, 16, 19, 38, 40, 41, 50, 55, 57, 61, 67, 71, 77, 78, 79, 81, 82, 83, 84, 85, 90, 93, 95, 97, 102, 105, 111, 119, 131, 133, 144, 149, 157, 159, 161, 180, 187, 195, 198, 204, 206, 209, 211, 213, 217, 218, 219, 222, 229, 231, 235, 240, 243, 245, 247, 249, 254, 258, 259, 262, 276, 280, 282, 285, 286, 297, 299, 300, 302, 303, 308, 317, 330, 331, 333, 339, 342, 359, 363, 365, 366, 376, 386, 388, 397, 398, 401, 403, 405, 424, 427, 432, 446, 452, 453, 477, 492, 496, 512, 517, 519, 526, 529, 530, 538, 540, 541, 551, 559, 561, 564, 570, 573, 574, 577, 580, 583, 585, 586, 599, 601, 606, 609, 611, 615, 624, 634, 643, 647, 651, 652, 663, 664, 672, 688, 689, 699, 700, 705, 716, 718, 720, 726, 729, 730, 731, 741, 745, 749, 752, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.intermediate.dense\n",
      "  Dead Neurons: [0, 1, 7, 19, 21, 26, 35, 39, 43, 46, 56, 59, 61, 65, 66, 69, 72, 80, 90, 98, 99, 100, 106, 110, 112, 114, 129, 133, 134, 135, 158, 163, 164, 176, 182, 183, 186, 188, 189, 194, 199, 206, 208, 221, 244, 254, 261, 262, 271, 273, 279, 286, 300, 309, 317, 318, 325, 326, 327, 329, 334, 336, 340, 343, 354, 359, 363, 377, 379, 382, 388, 394, 395, 401, 408, 409, 416, 418, 420, 429, 432, 439, 443, 448, 449, 465, 469, 474, 492, 496, 498, 503, 508, 510, 515, 517, 519, 521, 540, 544, 547, 549, 553, 564, 566, 577, 586, 589, 591, 592, 596, 602, 606, 613, 614, 627, 630, 634, 637, 638, 639, 641, 647, 650, 651, 654, 655, 657, 674, 675, 677, 684, 688, 696, 697, 709, 722, 727, 729, 743, 744, 748, 761, 769, 775, 780, 793, 801, 802, 809, 814, 818, 819, 826, 830, 832, 836, 840, 843, 844, 846, 848, 856, 860, 865, 876, 884, 898, 903, 904, 909, 912, 915, 917, 918, 927, 934, 938, 944, 945, 951, 954, 971, 972, 974, 976, 981, 982, 986, 993, 994, 1000, 1011, 1016, 1019, 1020, 1023, 1027, 1038, 1040, 1041, 1047, 1054, 1069, 1070, 1078, 1082, 1087, 1094, 1101, 1102, 1112, 1118, 1121, 1127, 1134, 1136, 1139, 1141, 1150, 1157, 1159, 1160, 1162, 1165, 1171, 1175, 1183, 1188, 1190, 1199, 1200, 1206, 1215, 1222, 1224, 1229, 1235, 1241, 1242, 1245, 1246, 1251, 1255, 1258, 1267, 1269, 1271, 1275, 1277, 1283, 1289, 1290, 1294, 1298, 1302, 1303, 1304, 1307, 1308, 1315, 1325, 1329, 1339, 1347, 1353, 1356, 1357, 1361, 1365, 1374, 1379, 1381, 1383, 1384, 1387, 1388, 1394, 1400, 1403, 1411, 1412, 1414, 1415, 1423, 1426, 1433, 1440, 1444, 1449, 1460, 1469, 1471, 1475, 1476, 1479, 1488, 1490, 1491, 1496, 1509, 1518, 1521, 1523, 1524, 1528, 1539, 1557, 1563, 1570, 1578, 1579, 1580, 1582, 1593, 1609, 1611, 1619, 1626, 1631, 1633, 1634, 1641, 1647, 1653, 1654, 1657, 1661, 1666, 1671, 1673, 1681, 1682, 1689, 1691, 1698, 1703, 1708, 1709, 1714, 1715, 1716, 1717, 1722, 1723, 1724, 1726, 1733, 1734, 1735, 1737, 1744, 1748, 1751, 1754, 1758, 1760, 1762, 1765, 1772, 1788, 1790, 1795, 1829, 1832, 1867, 1871, 1876, 1879, 1880, 1882, 1890, 1906, 1911, 1912, 1918, 1929, 1930, 1935, 1939, 1956, 1962, 1965, 1969, 1974, 1987, 1989, 1990, 1996, 2001, 2021, 2028, 2033, 2034, 2035, 2037, 2048, 2052, 2053, 2055, 2105, 2106, 2107, 2125, 2128, 2139, 2146, 2151, 2161, 2163, 2165, 2170, 2172, 2173, 2176, 2177, 2182, 2183, 2186, 2202, 2206, 2207, 2218, 2220, 2221, 2231, 2237, 2239, 2247, 2249, 2250, 2259, 2263, 2264, 2265, 2266, 2271, 2272, 2279, 2283, 2286, 2287, 2297, 2300, 2303, 2304, 2311, 2316, 2319, 2320, 2324, 2330, 2336, 2342, 2343, 2348, 2350, 2351, 2356, 2361, 2362, 2365, 2380, 2381, 2382, 2390, 2391, 2407, 2408, 2409, 2414, 2424, 2425, 2428, 2431, 2436, 2437, 2451, 2457, 2461, 2465, 2475, 2480, 2483, 2488, 2489, 2500, 2516, 2521, 2522, 2525, 2528, 2531, 2533, 2535, 2541, 2546, 2553, 2556, 2558, 2561, 2564, 2565, 2570, 2572, 2579, 2580, 2581, 2583, 2588, 2592, 2597, 2601, 2606, 2611, 2624, 2625, 2626, 2630, 2632, 2634, 2635, 2636, 2640, 2644, 2645, 2654, 2661, 2669, 2672, 2675, 2684, 2687, 2693, 2694, 2695, 2698, 2699, 2702, 2704, 2713, 2714, 2717, 2719, 2724, 2730, 2733, 2734, 2741, 2742, 2743, 2746, 2752, 2757, 2766, 2770, 2775, 2777, 2778, 2793, 2795, 2798, 2799, 2800, 2803, 2808, 2811, 2817, 2818, 2823, 2831, 2833, 2840, 2842, 2848, 2853, 2868, 2881, 2892, 2896, 2902, 2904, 2906, 2914, 2917, 2918, 2919, 2922, 2934, 2936, 2937, 2940, 2941, 2959, 2962, 2963, 2964, 2965, 2967, 2969, 2972, 2987, 2995, 3009, 3011, 3015, 3024, 3036, 3046, 3047, 3054, 3059, 3067, 3069]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.10.output.dense\n",
      "  Dead Neurons: [3, 4, 5, 10, 11, 13, 15, 16, 19, 38, 40, 41, 46, 47, 57, 63, 67, 73, 75, 77, 78, 82, 83, 85, 87, 90, 93, 97, 100, 102, 111, 112, 115, 119, 122, 124, 125, 131, 133, 144, 149, 157, 159, 185, 187, 188, 193, 195, 198, 204, 206, 211, 213, 217, 219, 223, 224, 232, 235, 240, 245, 247, 258, 259, 276, 278, 282, 285, 286, 287, 297, 299, 300, 302, 317, 324, 326, 330, 331, 333, 359, 361, 365, 366, 373, 376, 388, 397, 398, 403, 405, 406, 409, 417, 424, 426, 432, 446, 452, 453, 468, 477, 491, 496, 497, 498, 505, 511, 518, 519, 526, 530, 538, 540, 541, 551, 554, 559, 561, 564, 570, 573, 577, 580, 586, 587, 604, 611, 624, 634, 647, 651, 652, 653, 655, 656, 664, 672, 674, 684, 689, 695, 718, 720, 731, 733, 734, 735, 749, 750, 751, 752, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.query\n",
      "  Dead Neurons: [0, 39, 81, 82, 105, 107, 128, 132, 133, 134, 135, 139, 140, 142, 144, 155, 164, 165, 166, 167, 171, 172, 174, 175, 176, 178, 179, 182, 183, 184, 191, 194, 200, 201, 212, 218, 223, 232, 242, 243, 247, 253, 295, 321, 322, 323, 325, 327, 328, 331, 333, 334, 335, 338, 342, 344, 345, 346, 347, 352, 353, 356, 357, 358, 359, 360, 365, 367, 368, 370, 372, 375, 377, 378, 407, 454, 512, 513, 514, 515, 519, 520, 522, 535, 537, 546, 549, 552, 554, 565, 572, 583, 621, 623, 654, 662, 669, 678, 679, 686, 691, 697, 704, 705, 706, 707, 708, 709, 710, 711, 714, 715, 716, 719, 720, 721, 722, 724, 726, 727, 728, 729, 730, 731, 732, 733, 734, 736, 737, 738, 739, 740, 741, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 762, 763, 764, 765, 766, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.key\n",
      "  Dead Neurons: [0, 53, 81, 90, 92, 105, 113, 116, 117, 128, 129, 133, 147, 151, 173, 189, 194, 201, 202, 226, 229, 232, 238, 243, 252, 283, 284, 294, 320, 321, 322, 323, 324, 325, 326, 327, 332, 333, 334, 335, 336, 337, 340, 341, 342, 344, 345, 346, 347, 349, 352, 353, 354, 356, 358, 362, 364, 366, 368, 371, 372, 376, 378, 408, 411, 417, 457, 463, 484, 486, 507, 513, 514, 520, 535, 540, 550, 565, 572, 573, 577, 579, 593, 615, 635, 648, 650, 654, 659, 662, 669, 678, 679, 681, 686, 687, 691, 693, 697, 698, 699, 701, 703, 704, 706, 707, 708, 709, 710, 712, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 726, 728, 729, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 743, 745, 746, 747, 748, 749, 751, 752, 753, 754, 756, 758, 759, 761, 762, 763, 764, 765, 766, 767]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.self.value\n",
      "  Dead Neurons: [4, 17, 27, 29, 31, 32, 37, 39, 45, 47, 54, 63, 66, 68, 71, 73, 76, 77, 78, 81, 84, 85, 86, 87, 90, 91, 92, 98, 102, 103, 106, 108, 112, 113, 114, 115, 117, 118, 120, 123, 127, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 210, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 249, 250, 251, 252, 253, 254, 255, 320, 321, 323, 324, 325, 327, 328, 329, 330, 331, 332, 333, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 350, 352, 354, 356, 358, 359, 360, 361, 362, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 381, 383, 430, 528, 537, 566, 568, 574, 581, 721, 748]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.attention.output.dense\n",
      "  Dead Neurons: [1, 4, 5, 16, 31, 40, 41, 45, 50, 64, 69, 77, 78, 82, 83, 93, 97, 102, 107, 114, 121, 140, 145, 147, 148, 149, 154, 158, 162, 173, 177, 178, 183, 201, 204, 213, 217, 222, 223, 228, 234, 235, 240, 248, 250, 258, 259, 263, 267, 276, 280, 282, 285, 286, 292, 294, 297, 299, 301, 311, 315, 316, 317, 330, 331, 332, 349, 353, 354, 359, 363, 364, 365, 366, 379, 382, 384, 385, 388, 401, 406, 407, 413, 415, 424, 426, 434, 440, 443, 453, 456, 461, 473, 476, 477, 488, 490, 493, 496, 502, 512, 517, 521, 528, 530, 534, 537, 539, 541, 544, 551, 553, 567, 580, 585, 587, 591, 599, 600, 601, 611, 614, 615, 617, 620, 624, 627, 629, 637, 643, 644, 649, 651, 663, 664, 667, 669, 670, 679, 680, 687, 688, 695, 700, 715, 719, 722, 726, 731, 732, 734, 736, 739, 760]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.intermediate.dense\n",
      "  Dead Neurons: [4, 5, 7, 12, 19, 22, 30, 37, 38, 42, 45, 48, 50, 52, 63, 66, 69, 72, 76, 88, 89, 91, 95, 104, 112, 120, 121, 124, 125, 138, 139, 140, 143, 148, 150, 151, 152, 153, 157, 161, 162, 169, 170, 172, 179, 184, 190, 209, 213, 225, 229, 232, 238, 246, 251, 255, 256, 259, 262, 269, 270, 271, 272, 276, 285, 300, 307, 310, 315, 320, 322, 326, 334, 341, 343, 351, 361, 362, 368, 374, 375, 378, 381, 385, 392, 393, 407, 414, 420, 423, 425, 426, 433, 436, 438, 443, 450, 462, 468, 469, 476, 483, 489, 490, 494, 498, 502, 504, 508, 514, 517, 519, 522, 523, 539, 541, 542, 550, 553, 557, 571, 572, 574, 580, 592, 609, 620, 623, 628, 632, 634, 635, 636, 637, 648, 649, 650, 658, 659, 660, 668, 669, 672, 673, 674, 676, 696, 705, 721, 737, 740, 741, 754, 768, 770, 773, 779, 780, 789, 798, 800, 806, 813, 826, 828, 836, 840, 841, 848, 851, 862, 874, 879, 882, 883, 884, 889, 899, 900, 909, 910, 918, 923, 937, 941, 943, 944, 945, 948, 949, 951, 958, 963, 966, 967, 968, 970, 978, 985, 997, 1000, 1002, 1010, 1012, 1017, 1031, 1047, 1054, 1055, 1069, 1073, 1074, 1075, 1088, 1089, 1091, 1097, 1098, 1107, 1120, 1125, 1129, 1135, 1137, 1138, 1139, 1146, 1169, 1186, 1196, 1205, 1207, 1213, 1218, 1220, 1249, 1250, 1252, 1263, 1273, 1284, 1297, 1299, 1301, 1305, 1307, 1308, 1319, 1323, 1324, 1325, 1329, 1330, 1331, 1335, 1341, 1342, 1344, 1352, 1358, 1360, 1362, 1364, 1385, 1389, 1393, 1408, 1412, 1417, 1424, 1426, 1427, 1429, 1432, 1436, 1441, 1449, 1450, 1451, 1456, 1479, 1490, 1491, 1493, 1523, 1529, 1532, 1539, 1542, 1545, 1546, 1553, 1564, 1567, 1574, 1578, 1583, 1605, 1607, 1608, 1610, 1613, 1616, 1621, 1624, 1626, 1633, 1639, 1650, 1652, 1658, 1664, 1665, 1666, 1667, 1671, 1679, 1681, 1682, 1685, 1691, 1692, 1709, 1710, 1717, 1718, 1719, 1730, 1741, 1745, 1746, 1749, 1755, 1759, 1761, 1768, 1771, 1774, 1776, 1788, 1795, 1800, 1805, 1807, 1811, 1813, 1817, 1820, 1827, 1830, 1836, 1840, 1841, 1842, 1843, 1847, 1851, 1856, 1861, 1862, 1863, 1884, 1886, 1891, 1897, 1901, 1910, 1911, 1915, 1917, 1920, 1923, 1926, 1933, 1936, 1946, 1948, 1949, 1950, 1951, 1958, 1967, 1968, 1974, 1977, 1980, 1981, 1987, 1989, 2000, 2001, 2003, 2004, 2014, 2016, 2029, 2030, 2039, 2041, 2044, 2049, 2050, 2060, 2065, 2070, 2073, 2076, 2077, 2081, 2085, 2089, 2090, 2094, 2101, 2112, 2123, 2128, 2131, 2136, 2147, 2152, 2155, 2158, 2160, 2163, 2168, 2170, 2171, 2173, 2174, 2176, 2177, 2178, 2181, 2186, 2187, 2192, 2196, 2198, 2199, 2201, 2207, 2212, 2225, 2229, 2239, 2241, 2255, 2260, 2263, 2266, 2271, 2273, 2275, 2282, 2285, 2289, 2292, 2293, 2296, 2299, 2300, 2302, 2314, 2325, 2326, 2327, 2332, 2336, 2340, 2350, 2351, 2357, 2364, 2366, 2367, 2370, 2375, 2384, 2396, 2399, 2407, 2409, 2413, 2414, 2418, 2423, 2425, 2427, 2429, 2431, 2432, 2433, 2435, 2442, 2449, 2450, 2451, 2453, 2456, 2460, 2466, 2469, 2477, 2513, 2517, 2521, 2523, 2526, 2542, 2544, 2548, 2562, 2565, 2575, 2576, 2585, 2599, 2600, 2608, 2613, 2617, 2619, 2621, 2631, 2633, 2636, 2641, 2644, 2645, 2651, 2655, 2656, 2665, 2677, 2680, 2689, 2701, 2702, 2704, 2708, 2712, 2727, 2732, 2733, 2736, 2738, 2740, 2741, 2753, 2754, 2760, 2761, 2763, 2764, 2784, 2785, 2791, 2793, 2797, 2800, 2810, 2817, 2827, 2834, 2835, 2844, 2846, 2851, 2852, 2855, 2857, 2863, 2874, 2884, 2890, 2892, 2894, 2897, 2898, 2909, 2911, 2930, 2931, 2940, 2941, 2944, 2956, 2958, 2959, 2963, 2964, 2966, 2967, 2968, 2976, 2983, 2985, 2988, 2989, 2992, 2997, 3002, 3003, 3012, 3021, 3023, 3038, 3042, 3046, 3048, 3059, 3060, 3063]\n",
      "  Total Dead Neurons: 614\n",
      "--------------------------------------------------\n",
      "Layer: roberta.encoder.layer.11.output.dense\n",
      "  Dead Neurons: [3, 4, 5, 10, 11, 15, 16, 19, 28, 38, 40, 41, 47, 57, 60, 67, 73, 78, 81, 82, 83, 85, 90, 93, 95, 97, 102, 105, 111, 112, 115, 119, 122, 123, 124, 125, 131, 133, 144, 148, 149, 154, 157, 159, 187, 188, 193, 198, 204, 206, 211, 213, 219, 222, 229, 232, 235, 240, 245, 251, 258, 259, 265, 276, 278, 282, 285, 286, 287, 297, 299, 300, 309, 326, 330, 331, 359, 360, 361, 365, 366, 373, 376, 388, 396, 397, 398, 403, 406, 409, 417, 424, 426, 432, 440, 446, 452, 468, 473, 477, 490, 496, 498, 511, 518, 519, 530, 534, 538, 540, 554, 564, 573, 577, 580, 585, 587, 599, 604, 611, 613, 615, 624, 627, 634, 641, 647, 651, 652, 655, 656, 662, 664, 667, 672, 674, 684, 689, 694, 695, 702, 718, 720, 726, 731, 735, 741, 742, 749, 751, 752, 760, 764, 765]\n",
      "  Total Dead Neurons: 154\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_dead_neurons(model):\n",
    "    \"\"\"\n",
    "    Find neurons that have zero weights in all their incoming connections across the layers.\n",
    "\n",
    "    :param model: The model to evaluate.\n",
    "    :return: A dictionary where keys are layer names and values are lists of indices of dead neurons.\n",
    "    \"\"\"\n",
    "    dead_neurons = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            weight_matrix = module.weight.cpu().detach().numpy()\n",
    "            dead_neuron_indices = []\n",
    "\n",
    "            # Check each neuron (row in the weight matrix) for all-zero weights\n",
    "            for i, neuron_weights in enumerate(weight_matrix):\n",
    "                if not neuron_weights.any():  # If all weights are zero\n",
    "                    dead_neuron_indices.append(i)\n",
    "\n",
    "            if dead_neuron_indices:\n",
    "                dead_neurons[name] = dead_neuron_indices\n",
    "\n",
    "    return dead_neurons\n",
    "\n",
    "def print_dead_neurons(dead_neurons):\n",
    "    \"\"\"\n",
    "    Print the dead neurons identified in the model.\n",
    "\n",
    "    :param dead_neurons: A dictionary of dead neurons found by find_dead_neurons.\n",
    "    \"\"\"\n",
    "    for layer_name, neuron_indices in dead_neurons.items():\n",
    "        print(f\"Layer: {layer_name}\")\n",
    "        print(f\"  Dead Neurons: {neuron_indices}\")\n",
    "        print(f\"  Total Dead Neurons: {len(neuron_indices)}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the original model\n",
    "model_path = '/content/drive/MyDrive/trained_model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Find and print dead neurons in the original model\n",
    "print(\"Original Model Dead Neurons:\")\n",
    "dead_neurons_original = find_dead_neurons(model)\n",
    "print_dead_neurons(dead_neurons_original)\n",
    "\n",
    "# Assuming pruned_model is the pruned model, you can also find and print its dead neurons\n",
    "print(\"Pruned Model Dead Neurons:\")\n",
    "dead_neurons_pruned = find_dead_neurons(pruned_model)\n",
    "print_dead_neurons(dead_neurons_pruned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-oQHQyb0n7K"
   },
   "source": [
    "##  Student - teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "jwEuZEzrQKw3",
    "outputId": "691ce66e-c32e-4bb1-aebb-d8596638ea5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2158' max='12375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2158/12375 25:39 < 2:01:37, 1.40 it/s, Epoch 0.52/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12375' max='12375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12375/12375 2:48:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.021738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.023753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.017933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12375, training_loss=0.03647315346413866, metrics={'train_runtime': 10116.9585, 'train_samples_per_second': 19.568, 'train_steps_per_second': 1.223, 'total_flos': 2.6223776107536384e+16, 'train_loss': 0.03647315346413866, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pre-trained RoBERTa model as the teacher model\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/NLP_project/trained_model')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Initialize a smaller student model\n",
    "student_model = RobertaForSequenceClassification.from_pretrained('distilbert/distilroberta-base', num_labels=teacher_model.config.num_labels)\n",
    "\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# Load your datasets\n",
    "# train_dataset = load_dataset(data_set, split='train')\n",
    "# val_dataset = load_dataset(data_set, split='validation')\n",
    "\n",
    "# # Tokenize the datasets\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/NLP_project/distilled_model',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='/content/drive/MyDrive/NLP_project/logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Define the knowledge distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0, alpha=0.5):\n",
    "    # Soft targets\n",
    "    soft_targets = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    # Student outputs with softmax\n",
    "    student_softmax = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    # Loss calculation\n",
    "    loss = torch.nn.functional.kl_div(student_softmax, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
    "    return alpha * loss\n",
    "\n",
    "# Custom Trainer for Knowledge Distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs_student = model(**inputs)\n",
    "        student_logits = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = teacher_model(**inputs)\n",
    "            teacher_logits = outputs_teacher.logits\n",
    "\n",
    "        loss = distillation_loss(student_logits, teacher_logits)\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1njYjyZaNOIF"
   },
   "outputs": [],
   "source": [
    "# student_model.save_pretrained('/content/drive/MyDrive/NLP_project/student_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "8cwy0QZQNXx4",
    "outputId": "22d4e9f4-aaa6-4789-d4a8-6d8c7f207938"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2063' max='2063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2063/2063 02:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model - Validation Loss: 0.04337078332901001\n",
      "Student Model - Validation Accuracy: 99.03%\n",
      "Size (MB): 328.515929\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update\n",
    "trainer.model = student_model\n",
    "\n",
    "# Evaluate\n",
    "pruned_results = trainer.evaluate()\n",
    "print(f\"Student Model - Validation Loss: {pruned_results['eval_loss']}\")\n",
    "print(f\"Student Model - Validation Accuracy: {pruned_results['eval_accuracy'] * 100:.2f}%\")\n",
    "print_size_of_model(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Section"
   ],
   "metadata": {
    "id": "FYKiCTlTU2RW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nj6h44ClI04u",
    "outputId": "378e4304-e4b5-49e3-e192-fa3880bfde93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRUNED RoBERTA PARAMETER: \n",
      "\n",
      "Total parameters: 124647170\n",
      "Trainable parameters: 124647170\n",
      "Non-trainable parameters: 0\n",
      "\n",
      " RRoBERTA PARAMETER: \n",
      "\n",
      "Total parameters: 124647170\n",
      "Trainable parameters: 124647170\n",
      "Non-trainable parameters: 0\n",
      "\n",
      " dislation PARAMETER: \n",
      "\n",
      "Total parameters: 82119938\n",
      "Trainable parameters: 82119938\n",
      "Non-trainable parameters: 0\n",
      "\n",
      " quantisize PARAMETER: \n",
      "\n",
      "Total parameters: 39037440\n",
      "Trainable parameters: 39037440\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "print('PRUNED RoBERTA PARAMETER: \\n')\n",
    "count_parameters(nlp.model)\n",
    "print('\\n RRoBERTA PARAMETER: \\n')\n",
    "count_parameters(model)\n",
    "print('\\n dislation PARAMETER: \\n')\n",
    "count_parameters(RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/NLP_project/student_model'))\n",
    "print('\\n quantisize PARAMETER: \\n')\n",
    "count_parameters(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print_size_of_model(RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/NLP_project/student_model'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meAjCajhU1Vn",
    "outputId": "4849d3d2-2c8f-4811-b2f2-de2e52982ee5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model size: 313.30 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print_size_of_model(RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/NLP_project/trained_model'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3knrFZ4UVJDr",
    "outputId": "4dcaba6d-29c1-45fc-c8e6-08eba83cb16b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model size: 475.55 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print_size_of_model(quantized_model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBEGzYKrVWB2",
    "outputId": "86969f72-c5f8-4409-f9c5-2a7496a1befb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model size: 230.92 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print_size_of_model(pruned_model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hb4GqXYMWsCm",
    "outputId": "1d138cca-82ae-4cbf-e873-5c72d1fba40a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model size: 475.56 MB\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0102bfbd68bd411194eb6a9233464607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05e35f412e7d4d5884265d62641e4f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0babbf5feec1445a81d231151e0d5e77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18199459dda04eb9b3f13dcd645f2d93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "196ef8bf48a04a6993efb0db48edde60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19bd2b6b3839417a8a456e68a7f474ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ceaf585a905411a89485a8d3a940aa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0babbf5feec1445a81d231151e0d5e77",
      "placeholder": "​",
      "style": "IPY_MODEL_c295b77221bd42fcbeb4cd0d1d23b61d",
      "value": "vocab.json: 100%"
     }
    },
    "1df1efb3eb6e40c396ae8040c2e7fb68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e33072bcbd14bc8bf44d21a67ac64c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "211445502d934ce9a32d45f07443c3b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "242817432ec64c65ae0092eaa57b0b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c888cefa52a44907858d7ef96e9c4046",
       "IPY_MODEL_793cbea070ab4faca6e3e17eb129e5fb",
       "IPY_MODEL_8ff25d74ba514708b3ee362b56ef3e2d"
      ],
      "layout": "IPY_MODEL_211445502d934ce9a32d45f07443c3b4"
     }
    },
    "256e6aa44ba94e82be6be54df640e7e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f90152cb8c415797a8c85a04494566": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2644c577cc08493f938356779d39ad7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3032a4be86884bb9aaebf9bf8ca64060": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36f5e12650dc414d9af98c8146f478e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d1540c6e61d4bebbe6f27a3019772cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "472db68a7cf8426597c768d83047f474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98f63712498f4bf698728a899045b915",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a88ecf8d445449ecae942377e7e4edf2",
      "value": 456318
     }
    },
    "4bb4894ac49b4073b5add5cf60310ad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e14968a1916242f7b7ea9f1226f9da38",
      "placeholder": "​",
      "style": "IPY_MODEL_18199459dda04eb9b3f13dcd645f2d93",
      "value": "merges.txt: 100%"
     }
    },
    "4ddae419281b4414be628c58c88bc444": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fd22bc2d2b34e38ac0d3dee905b6ffc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53fe61b3609b43daa7f9180121a350ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_256e6aa44ba94e82be6be54df640e7e7",
      "placeholder": "​",
      "style": "IPY_MODEL_60b93f1ad3d54b399a2b41c9d4677ff8",
      "value": " 481/481 [00:00&lt;00:00, 41.5kB/s]"
     }
    },
    "59b49e23d22f4316b87c61687aea8542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e489a97a30c40aa8ee8b24e3ee0f5c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f77d7db3b5c42b49f0a30f8579ed8a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f00b599c85a4415a0c892ba28ae3d3f",
      "placeholder": "​",
      "style": "IPY_MODEL_73cc3ae65cf6442697ff3347428cca75",
      "value": "tokenizer.json: 100%"
     }
    },
    "60b93f1ad3d54b399a2b41c9d4677ff8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "610eac5e5abc403aa57eaeca7df11e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddae419281b4414be628c58c88bc444",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_daf3e5ceb7204da08b9d751b9aeeee82",
      "value": 898823
     }
    },
    "68362ff0b054417c83aa7b0c31379a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ceaf585a905411a89485a8d3a940aa6",
       "IPY_MODEL_610eac5e5abc403aa57eaeca7df11e35",
       "IPY_MODEL_7295d19ef7ca4384a7b7501f8d29e915"
      ],
      "layout": "IPY_MODEL_59b49e23d22f4316b87c61687aea8542"
     }
    },
    "684fe0013ef344f28cb39f57f5dc2e41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69b492f4188941828742aa6dd757aabc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d02e0d72c454d248d662c7fc9458234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f75e33e001f5498b8d5d553ace5cb452",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25f90152cb8c415797a8c85a04494566",
      "value": 481
     }
    },
    "6e0a14a8ecd84bcd9ec70e21deb9e50d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec8a66d29a9b434ba3eb197a43a6c949",
       "IPY_MODEL_6d02e0d72c454d248d662c7fc9458234",
       "IPY_MODEL_53fe61b3609b43daa7f9180121a350ea"
      ],
      "layout": "IPY_MODEL_2644c577cc08493f938356779d39ad7a"
     }
    },
    "6f00b599c85a4415a0c892ba28ae3d3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7295d19ef7ca4384a7b7501f8d29e915": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36f5e12650dc414d9af98c8146f478e1",
      "placeholder": "​",
      "style": "IPY_MODEL_a0ffda0894874241940c84cc5e6c6a8c",
      "value": " 899k/899k [00:00&lt;00:00, 19.4MB/s]"
     }
    },
    "73cc3ae65cf6442697ff3347428cca75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7683b095694347bf83fc4d66e6b7f8b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76bf606aa53e43039fb6f9cf4e15018e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "793cbea070ab4faca6e3e17eb129e5fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae2cb29a2b324629b6bf521b24ef11ee",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82d59f5b21ba46a8928c1c2ebe3a00b1",
      "value": 25
     }
    },
    "82d59f5b21ba46a8928c1c2ebe3a00b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "845ff7aa240e420ba636f1e73aab0d9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ff25d74ba514708b3ee362b56ef3e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_196ef8bf48a04a6993efb0db48edde60",
      "placeholder": "​",
      "style": "IPY_MODEL_05e35f412e7d4d5884265d62641e4f88",
      "value": " 25.0/25.0 [00:00&lt;00:00, 1.95kB/s]"
     }
    },
    "968cabbcbdff41b4bd3afd2911e7b2ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98f63712498f4bf698728a899045b915": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ffda0894874241940c84cc5e6c6a8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a88ecf8d445449ecae942377e7e4edf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aaa9bc10a3454789825a732a098b929e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e33072bcbd14bc8bf44d21a67ac64c4",
      "max": 498818054,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d1540c6e61d4bebbe6f27a3019772cd",
      "value": 498818054
     }
    },
    "ad1587a6f64e4baf8ca640d529c63aa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae2cb29a2b324629b6bf521b24ef11ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5689b9f66ef46e2bd34c92d3ebb3077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8f1931f21c8495e86025391daeb3184",
      "placeholder": "​",
      "style": "IPY_MODEL_ccd317d11b9041bd9fef0a5c644a2e10",
      "value": " 456k/456k [00:00&lt;00:00, 922kB/s]"
     }
    },
    "bab3ca3d70634690a60fd3e422cdf7b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845ff7aa240e420ba636f1e73aab0d9b",
      "placeholder": "​",
      "style": "IPY_MODEL_4fd22bc2d2b34e38ac0d3dee905b6ffc",
      "value": "model.safetensors: 100%"
     }
    },
    "c295b77221bd42fcbeb4cd0d1d23b61d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8870cbc209a45299e74d4028e47b7dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bab3ca3d70634690a60fd3e422cdf7b3",
       "IPY_MODEL_aaa9bc10a3454789825a732a098b929e",
       "IPY_MODEL_da775165bff14fdea5a023810e3dd15c"
      ],
      "layout": "IPY_MODEL_3032a4be86884bb9aaebf9bf8ca64060"
     }
    },
    "c888cefa52a44907858d7ef96e9c4046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_968cabbcbdff41b4bd3afd2911e7b2ee",
      "placeholder": "​",
      "style": "IPY_MODEL_0102bfbd68bd411194eb6a9233464607",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "c8f1931f21c8495e86025391daeb3184": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccd317d11b9041bd9fef0a5c644a2e10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccdee859fdcf4836b4155cf959a79980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceafc6bdc70c48e093acb0b068c6e10a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4bb4894ac49b4073b5add5cf60310ad8",
       "IPY_MODEL_472db68a7cf8426597c768d83047f474",
       "IPY_MODEL_b5689b9f66ef46e2bd34c92d3ebb3077"
      ],
      "layout": "IPY_MODEL_684fe0013ef344f28cb39f57f5dc2e41"
     }
    },
    "da775165bff14fdea5a023810e3dd15c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1df1efb3eb6e40c396ae8040c2e7fb68",
      "placeholder": "​",
      "style": "IPY_MODEL_69b492f4188941828742aa6dd757aabc",
      "value": " 499M/499M [00:02&lt;00:00, 247MB/s]"
     }
    },
    "daf3e5ceb7204da08b9d751b9aeeee82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e14968a1916242f7b7ea9f1226f9da38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e387f74302364bbdbf63aae4e648ad8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f77d7db3b5c42b49f0a30f8579ed8a1",
       "IPY_MODEL_e99ef5310adb43fca90d1300149aff07",
       "IPY_MODEL_edecbe4e701348e49f7c4d461554e40d"
      ],
      "layout": "IPY_MODEL_f0434966f0944840bba888b24fc05024"
     }
    },
    "e99ef5310adb43fca90d1300149aff07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccdee859fdcf4836b4155cf959a79980",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76bf606aa53e43039fb6f9cf4e15018e",
      "value": 1355863
     }
    },
    "ec8a66d29a9b434ba3eb197a43a6c949": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19bd2b6b3839417a8a456e68a7f474ff",
      "placeholder": "​",
      "style": "IPY_MODEL_ad1587a6f64e4baf8ca640d529c63aa2",
      "value": "config.json: 100%"
     }
    },
    "edecbe4e701348e49f7c4d461554e40d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e489a97a30c40aa8ee8b24e3ee0f5c2",
      "placeholder": "​",
      "style": "IPY_MODEL_7683b095694347bf83fc4d66e6b7f8b8",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 63.5MB/s]"
     }
    },
    "f0434966f0944840bba888b24fc05024": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f75e33e001f5498b8d5d553ace5cb452": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
